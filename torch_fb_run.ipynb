{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_fb_run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_lm4LjOZQTmf8kbrx2GD9eeJK5xNc5sC",
      "authorship_tag": "ABX9TyMQv6KeO8vBZS8R77Hw3SnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c9c72559dcf4197a4a504166dddb085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f20a11987fdf492fbcdd9d0e2e4eb243",
              "IPY_MODEL_0365ef81a92d4c06b553bdaec8bc1ad2",
              "IPY_MODEL_55a36b46ee3d4b0786d5c38b90df80f2"
            ],
            "layout": "IPY_MODEL_5d33ac5ac67442d7a74095c270f98965"
          }
        },
        "f20a11987fdf492fbcdd9d0e2e4eb243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c72bbc646e64151910101b5f822b2a4",
            "placeholder": "​",
            "style": "IPY_MODEL_6d08b866c2df420b9e4b5d19e115a8b4",
            "value": "100%"
          }
        },
        "0365ef81a92d4c06b553bdaec8bc1ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74f7165234ba4cf681cfe58403db54c1",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1a675f2405e48a4bbe0c800c2bb61d1",
            "value": 102530333
          }
        },
        "55a36b46ee3d4b0786d5c38b90df80f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e672e1eebba418fa6a2ad0e030f7c75",
            "placeholder": "​",
            "style": "IPY_MODEL_8a964add036f45a08455c84aa19d5cad",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 87.4MB/s]"
          }
        },
        "5d33ac5ac67442d7a74095c270f98965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c72bbc646e64151910101b5f822b2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d08b866c2df420b9e4b5d19e115a8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f7165234ba4cf681cfe58403db54c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a675f2405e48a4bbe0c800c2bb61d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e672e1eebba418fa6a2ad0e030f7c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a964add036f45a08455c84aa19d5cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajj8866/facebook_mkt/blob/main/torch_fb_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNk95wemVTg-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import shutil\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "from pathlib import Path\n",
        "print('Root folder director')\n",
        "print(Path.cwd())\n",
        "print(Path.home())\n",
        "print(os.listdir())\n",
        "os.chdir('./drive/MyDrive/facebook_mkt')\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_img = zipfile.ZipFile('images.zip')"
      ],
      "metadata": {
        "id": "D3popzID9uwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_img.extractall()"
      ],
      "metadata": {
        "id": "5-0ROYYxBY9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_img.close()"
      ],
      "metadata": {
        "id": "KP9uqCjhBqVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchbearer\n",
        "!pip install XlsxWriter\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_fchsIcQ-LU",
        "outputId": "7fcfad39-dd2a-4668-f2f8-4a1dbe9f7a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchbearer\n",
            "  Downloading torchbearer-0.5.3-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchbearer) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->torchbearer) (4.2.0)\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.0.3\n",
            "/content/drive/MyDrive/facebook_mkt\n",
            "['.gitignore', '.DS_Store', 'runs', 'images.zip', '__MACOSX', 'data_files', 'images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs\n",
        "# !cp 'images.zip'\n",
        "# !unzip -q images.zip\n",
        "# !rm images.zip\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import true\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from itertools import product\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import json \n",
        "import torchvision.transforms as transforms\n",
        "import re\n",
        "from PIL import Image\n",
        "import multiprocessing\n",
        "import torchvision\n",
        "from skimage import io\n",
        "from skimage import img_as_float\n",
        "from skimage.filters import sobel\n",
        "from skimage.color import rgb2gray\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchbearer import Trial\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision.transforms import Normalize, ToPILImage, ToTensor\n",
        "from torchbearer.callbacks import TensorBoard\n",
        "from torch.nn import Module\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "from torchvision import models, datasets\n",
        "import copy\n",
        "import time\n",
        "from tensorboard import notebook\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "class CleanData:\n",
        "    def __init__(self, tab_names = ['Products']) -> None:\n",
        "        self.tab_names = tab_names\n",
        "        maj_unique_cats = ['Home & Garden ', 'Baby & Kids Stuff ', 'DIY Tools & Materials ', 'Music, Films, Books & Games ', 'Phones, Mobile Phones & Telecoms ', 'Clothes, Footwear & Accessories ', 'Other Goods ', 'Health & Beauty ', 'Sports, Leisure & Travel ', 'Appliances ', 'Computers & Software ','Office Furniture & Equipment ', 'Video Games & Consoles ']\n",
        "        self.major_map_decoder = dict(enumerate(maj_unique_cats))\n",
        "        self.major_map_encoder = {val: key for key, val in self.major_map_decoder.items()}\n",
        "        if 'data_files' not in os.listdir():\n",
        "            os.mkdir(Path(Path.cwd(), 'data_files'))\n",
        "        self.table_dict = {}\n",
        "        for table in tab_names:\n",
        "            self.table_dict[table] = pd.read_json(Path(Path.cwd(),'data_files', table+'.json'))\n",
        "            self.table_dict[table].dropna(inplace = True)\n",
        "            if 'price' in self.table_dict[table].columns:\n",
        "                self.table_dict[table]['price'] = self.table_dict[table][self.table_dict[table]['price'] != 'N/A'.strip()]['price']\n",
        "                self.table_dict[table]['price'] = self.table_dict[table]['price'].str.replace(',', '').str.strip('£').str.strip(' ').astype(np.float32)\n",
        "                self.table_dict[table] = self.table_dict[table][np.round(self.table_dict[table]['price']) != 0]\n",
        "            if 'category' in self.table_dict[table].columns:\n",
        "                self.expand_category(df=table)\n",
        "\n",
        "    \n",
        "    def try_merge(self, df_list):\n",
        "        '''\n",
        "        Combines dataframes passed in into a single dataframe\n",
        "\n",
        "        Parameters:\n",
        "        df_list: Must contain dataframes within self.table_dict passed in as a list\n",
        "        '''\n",
        "        if isinstance(self.tab_names, str):\n",
        "            print('Method not valid when class instantiated with tab_names as type string')\n",
        "        else:\n",
        "            self.new_df = pd.DataFrame(columns = self.table_dict[df_list[0]].columns)\n",
        "            for i in df_list:\n",
        "                self.new_df = pd.concat([self.new_df, self.table_dict[i]], axis=0)\n",
        "        self.table_dict['combined'] = self.new_df\n",
        "        self.table_dict['combined'].dropna(inplace=True)\n",
        "        return self.table_dict['combined']\n",
        "    \n",
        "    def get_na_vals(self, df):\n",
        "        print(f'The following NA values exist if dataframe {df}')\n",
        "        return self.table_dict[df][self.table_dict[df].isna().any(axis=1)]\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        if isinstance(self.tab_names, str):\n",
        "            print(self.df.columns)\n",
        "            print('\\nTable Name: ', self.tab_names, 'With columns:')\n",
        "            return ' | '.join(self.df.columns)\n",
        "        else:\n",
        "            print('\\n')\n",
        "            print('Total of ', f'{len(self.table_dict)} tables')\n",
        "            return '\\n'.join([f'Table Name: {i}: \\n' f'Columns | {\" | \".join(j.columns)} \\n' for i, j in self.table_dict.items()])\n",
        "\n",
        "    def to_excel(self):\n",
        "        for i, j in self.table_dict.items():\n",
        "            ex_writer = pd.ExcelWriter(f'data_files/{i}.xlsx', engine='xlsxwriter')\n",
        "            with ex_writer as writer:\n",
        "                j.to_excel(writer, sheet_name=i)\n",
        "    \n",
        "    def cat_set(self, df = 'Products',cat_col = 'major_category'):\n",
        "        return self.table_dict[df][cat_col].nunique()\n",
        "    \n",
        "    def expand_category(self, df = 'Products'):\n",
        "        self.major_encoder = LabelEncoder()\n",
        "        self.minor_encoder = LabelEncoder()\n",
        "        self.table_dict[df]['major_category'] = self.table_dict[df]['category'].str.split('/').apply(lambda i: i[0])\n",
        "        self.table_dict[df]['minor_category'] = self.table_dict[df]['category'].str.split('/').apply(lambda i: i[1])\n",
        "        self.table_dict[df] = self.table_dict[df][self.table_dict[df]['major_category'] != 'N'.strip()]\n",
        "        self.table_dict[df]['major_category_encoded'] = self.table_dict[df]['major_category'].map(self.major_map_encoder)\n",
        "        self.table_dict[df]['minor_category_encoded'] = self.minor_encoder.fit_transform(self.table_dict[df]['minor_category'])\n",
        "        return self.table_dict[df]\n",
        "    \n",
        "    def inverse_transform(self, input_array, major_minor = 'minor'):\n",
        "        category_dict = {'major': self.major_encoder, 'minor': self.minor_encoder}\n",
        "        try:\n",
        "            return category_dict[major_minor].inverse_transform(input_array)\n",
        "        except TypeError:\n",
        "            return category_dict[major_minor].inverse_transform(input_array.numpy())\n",
        "    \n",
        "    \n",
        "    def sum_by_cat(self, df= 'Products', quant = 0.95):\n",
        "        data = self.expand_category(df)\n",
        "        major = data.groupby('major_category')['price'].describe()\n",
        "        print('Price Statistics Grouped by Major Category')\n",
        "        print(major)\n",
        "        major_cat_list = major.index.tolist()\n",
        "        #sns.boxplot(data=data, x = 'major_category', y = 'price')\n",
        "        products_df = data.loc[:, ['major_category', 'minor_category', 'price']]\n",
        "        for i in major_cat_list:\n",
        "            prod_plot = products_df.loc[products_df['major_category'] == i]\n",
        "            # print(prod_plot['price'].quantile([quant]))\n",
        "            # print(type(prod_plot['price'].quantile([quant][0])))\n",
        "            # print('Number of observations with price more than the 99th quantile: ', len(prod_plot[prod_plot['price'] > prod_plot['price'].quantile([quant][0])]))\n",
        "            # sns.boxplot(data=prod_plot, x='major_category', y='price')\n",
        "            # plt.show()\n",
        "            sns.boxplot(data=prod_plot[prod_plot['price']<prod_plot['price'].quantile([quant][0])], x = 'major_category', y = 'price')\n",
        "            plt.show()\n",
        "\n",
        "    def trim_data(self, df= 'Products', quant = 0.95):\n",
        "        self.table_dict[df] = self.table_dict[df][self.table_dict[df]['price'] > self.table_dict[df]['price'].quantile([quant])]\n",
        "        return self.table_dict[df]\n",
        "\n",
        "    @classmethod\n",
        "    def allTables(cls):\n",
        "        json_list = []\n",
        "        json_regex = re.compile(r'(.*).json$')\n",
        "        for i in os.listdir(Path(Path.cwd(), 'data_files')):\n",
        "            if re.search(json_regex, i) is not None:\n",
        "                json_list.append(re.search(json_regex, i).group(1))\n",
        "        print(json_list)\n",
        "        return cls(tab_names = json_list)\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "class CleanImages(CleanData):\n",
        "    def __init__(self, tab_names=['Images']) -> None:\n",
        "        super().__init__(tab_names)\n",
        "        self.df = self.table_dict[tab_names[0]].copy()\n",
        "        self.csv_df = None\n",
        "\n",
        "    def img_clean_pil(self, size = 512, mode = 'RGB'):\n",
        "        image_re = re.compile(r'(.*)\\.jpg')\n",
        "        os.chdir(Path(Path.cwd(), 'images'))\n",
        "        # os.chdir(Path(Path.cwd(), 'images'))\n",
        "        t = 0\n",
        "        for i in os.listdir():\n",
        "            if re.findall(image_re, i) != []:\n",
        "                try:\n",
        "                    temp_image = Image.open(i)\n",
        "                    black_back = Image.new(size=(size, size), mode=temp_image.mode) #, mode=mode\n",
        "                    curr_size = temp_image.size\n",
        "                    max_dim = max(temp_image.size)\n",
        "                    scale_fact = size / max_dim\n",
        "                    resized_image_dim = (int(scale_fact*curr_size[0]), int(scale_fact*curr_size[1]))\n",
        "                    updated_image = temp_image.resize(resized_image_dim)\n",
        "                    black_back.paste(updated_image, ((size- resized_image_dim[0])//2, (size- resized_image_dim[1])//2))\n",
        "                    black_back = black_back.convert(mode)\n",
        "                    t += 1\n",
        "                    black_back.save(i)\n",
        "                except Exception:\n",
        "                    print(i)\n",
        "                    with open('invalid_file.json', 'w') as wrong_form:\n",
        "                        json.dump(i, wrong_form)\n",
        "                    os.remove(i)\n",
        "                    pass\n",
        "        print(t)\n",
        "        os.chdir(Path(Path.cwd().parents[0]))\n",
        "\n",
        "    def img_clean_sk(self, normalize = False):\n",
        "        image_re = re.compile(r'(.*)\\.jpg')\n",
        "        img = []\n",
        "        img_dim_list = []\n",
        "        img_id = []\n",
        "        image_array = []\n",
        "        img_channels = []\n",
        "        img_num_features = []\n",
        "        img_mode = []\n",
        "        os.chdir(Path(Path.cwd(), 'images'))\n",
        "        for im in os.listdir():\n",
        "            if re.findall(image_re, im) != []:\n",
        "                img.append(im)\n",
        "                image = io.imread(im)\n",
        "                if normalize == True:\n",
        "                    image = img_as_float(image)\n",
        "                img_id.append(re.search(image_re, im).group(1))\n",
        "                image_array.append(image)\n",
        "                img_dim_list.append(image.shape)\n",
        "                if len(image.shape) == 3:\n",
        "                    img_num_features.append(image.shape[2])\n",
        "                else:\n",
        "                    img_num_features.append(1)\n",
        "                img_channels.append(len(image.shape))\n",
        "                img_mode.append(Image.open(im).mode)\n",
        "        os.chdir(Path(Path.cwd().parents[0]))\n",
        "        self.image_frame = pd.DataFrame(data={'image_id': img_id, 'image': img,'image_array': image_array,'image_shape': img_dim_list, 'mode': img_mode})\n",
        "        return self.image_frame\n",
        "    \n",
        "    def to_excel(self, df):\n",
        "        df.to_excel(Path(Path.cwd(), 'data_files','Cleaned_Images.xlsx'), sheet_name = 'images')\n",
        "\n",
        "    def merge_images(self):\n",
        "        self.df.rename({'id': 'image_id', 'product_id': 'id'}, axis=1, inplace=True)\n",
        "        self.final_df = self.image_frame.merge(self.df, on='image_id', how='inner', validate='one_to_many')\n",
        "        #print(self.final_df.head())\n",
        "        return self.final_df\n",
        "    \n",
        "    def edge_detect(self):\n",
        "        try:\n",
        "            self.image_frame['edge_array'] = self.image_frame['image_array'].copy().apply(lambda i: sobel(rgb2gray(i)))\n",
        "        except: \n",
        "            self.image_frame['edge_array'] = self.image_frame['image_array'].copy().apply(lambda i: sobel(i))\n",
        "        return self.image_frame\n",
        "\n",
        "\n",
        "    def total_clean(self, normalize=False, mode = 'RGB', size = 224):\n",
        "        self.img_clean_pil(mode=mode, size=size)\n",
        "        self.img_clean_sk(normalize=normalize)\n",
        "        self.edge_detect()\n",
        "        self.merge_images()\n",
        "        return self.final_df\n",
        "    \n",
        "    def show_random_images(self, col, size, fig_height= 15, fig_width=10):\n",
        "        grid = GridSpec(nrows = size, ncols = size)\n",
        "        fig = plt.figure(figsize=(fig_height, fig_width))\n",
        "        for i, j in product(range(size), range(size)):\n",
        "            fig.add_subplot(grid[i, j]).imshow(self.final_df[col].iloc[np.random.randint(low=0, high=len(self.final_df)-1)])\n",
        "        plt.show()\n",
        "\n",
        "    def describe_data(self, df):\n",
        "        print('\\n')\n",
        "        print('Data frame columnn information')\n",
        "        print(df.info())\n",
        "        print('\\n')\n",
        "        print('#'*20)\n",
        "        print('Dataframe statistical metrics')\n",
        "        #print(df.describe())\n",
        "        print('#'*20)\n",
        "        print('Array and shape')\n",
        "        print(df['image_shape'].unique())\n",
        "        print(df['image_shape'].value_counts())\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "class MergedData:\n",
        "    def __init__(self):\n",
        "        img_class = CleanImages()\n",
        "        prod_class = CleanData(tab_names=['Products'])\n",
        "        self.major_map_encoder = prod_class.major_map_encoder\n",
        "        self.major_map_decoder = prod_class.major_map_decoder\n",
        "        self.prod_frame = prod_class.table_dict['Products'].copy()\n",
        "        self.img_df = img_class.total_clean()\n",
        "        self.merged_frame = self.img_df.merge(self.prod_frame, left_on='id', right_on='id')\n",
        "    \n",
        "    def to_pickle(self):\n",
        "        self.merged_frame.to_pickle(Path(Path.cwd(), 'merged_data.pkl'))\n",
        "    \n",
        "    def get_val_counts(self):\n",
        "        return {'products': self.prod_frame, 'images': self.img_df, 'all': self.merged_frame}\n",
        "      \n",
        "#############################################################################################\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, transformer = transforms.Compose([ToTensor()]), X = 'image_array', y = 'major_category_encoded', img_dir = Path(Path.cwd(), 'images'), img_size=224, train_proportion = 0.8, is_test = False):\n",
        "        '''\n",
        "        X: Can be either 'image' if dataset to be instantiated using image object or 'image_array' if dataset to be instantiated using numpy array \n",
        "        y: Can be either 'major_category_encoded' or 'minor_category_encoded'\n",
        "        '''\n",
        "        self.img_inp_type = X\n",
        "        self.transformer = transformer\n",
        "        self.img_dir = img_dir\n",
        "        self.img_size = img_size\n",
        "        merge_class = MergedData()\n",
        "        merged_df = merge_class.merged_frame\n",
        "        filtered_df = merged_df.loc[:, ['image_id', X, re.sub(re.compile('_encoded$'), '', y), y]].copy()\n",
        "        filtered_df.dropna(inplace=True)\n",
        "        print(filtered_df[y].value_counts())\n",
        "        print(filtered_df[re.sub(re.compile('_encoded$'), '', y)].value_counts())\n",
        "        train_end = int(len(filtered_df)*train_proportion)\n",
        "        if is_test == False:\n",
        "            filtered_df = filtered_df.iloc[:train_end]\n",
        "        elif is_test == True:\n",
        "            filtered_df = filtered_df.iloc[train_end:]\n",
        "        else:\n",
        "            pass\n",
        "        self.dataset_size = len(filtered_df)\n",
        "        self.all_data = filtered_df\n",
        "        print('Total observations in remaining dataset: ', len(filtered_df))\n",
        "        self.y = torch.tensor(filtered_df[y].values)\n",
        "        self.X = filtered_df[X].values\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        if self.img_inp_type == 'image':\n",
        "            try:\n",
        "                self.X[idx] =  Image.open(os.path.join(self.img_dir, self.X[idx]))\n",
        "                if self.transformer is not None:\n",
        "                    self.X[idx] = self.transformer(self.X[idx])\n",
        "            except TypeError:\n",
        "                self.X[idx] = self.X[idx]\n",
        "        elif self.img_inp_type == 'image_array':\n",
        "            try:\n",
        "                # self.X[idx] = torch.from_numpy(np.transpose(self.X[idx], (2,1,0)))\n",
        "                if self.transformer is not None:\n",
        "                    self.X[idx] = self.transformer(self.X[idx])\n",
        "            except TypeError:\n",
        "                self.X[idx] = self.X[idx]\n",
        "        else:\n",
        "            self.X[idx] = self.X[idx]        \n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "#############################################################################################\n",
        "\n",
        "pd.set_option('display.max_colwidth', 400)\n",
        "pd.set_option('display.max_columns', 15)\n",
        "pd.set_option('display.max_rows', 40)\n",
        "plt.rc('axes', titlesize=12)\n",
        "\n",
        "res_model = models.resnet50(pretrained=True)\n",
        "for param in res_model.parameters():\n",
        "    param.requires_grad = False\n",
        "res_model.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=512, bias=True), nn.ReLU(inplace=True), nn.Dropout(p=0.2), nn.Linear(in_features=512, out_features=64), nn.Linear(in_features=64, out_features=13))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "res_model.to(device)\n",
        "\n",
        "opt = optim.SGD\n",
        "optimizer =  opt(res_model.parameters(), lr=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[5, 10, 15, 20, 25, 30], gamma=0.3) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def get_loader(img = 'image_array',batch_size=35, split_in_dataset = False, train_prop = 0.8):\n",
        "    train_transformer = transforms.Compose([transforms.RandomRotation(40), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    test_transformer = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    if split_in_dataset == True:\n",
        "        train_dataset = Dataset(transformer=train_transformer, X=img, img_size=224, is_test=False, train_proportion=train_prop)\n",
        "        test_dataset = Dataset(transformer=test_transformer, X=img, img_size=224, is_test=True, train_proportion=train_prop)\n",
        "        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "        test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n",
        "        data_loader_dict = {'train': train_loader, 'eval': test_loader}\n",
        "        return train_dataset.dataset_size, test_dataset.dataset_size, data_loader_dict\n",
        "    else:\n",
        "        image_datsets= Dataset(transformer=test_transformer, X = img, img_size=224, is_test=None)\n",
        "        train_end = int(train_prop*image_datsets.dataset_size)\n",
        "        train_dataset, test_dataset = random_split(image_datsets, lengths=[len(image_datsets.all_data.iloc[:train_end]), len(image_datsets.all_data.iloc[train_end:])])\n",
        "        dataset_dict = {'train': train_dataset, 'eval': test_dataset}\n",
        "        data_loader_dict = {i: DataLoader(dataset_dict[i], batch_size=batch_size, shuffle=True) for i in ['train', 'eval']}\n",
        "        return len(image_datsets.all_data.iloc[:train_end]), len(image_datsets.all_data.iloc[train_end:]), data_loader_dict\n",
        "    \n",
        "prod_dum = CleanData()\n",
        "class_dict = prod_dum.major_map_encoder.keys()\n",
        "classes = list(class_dict)\n",
        "class_values = prod_dum.major_map_encoder.values()\n",
        "class_encoder = prod_dum.major_map_encoder\n",
        "\n",
        "\n",
        "'''Tensorboard Function for Showing Images'''\n",
        "def show_image(input_ten_orig):\n",
        "    input_ten = torch.clone(input_ten_orig)\n",
        "    inv_normalize_array = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255])\n",
        "    inv_normalize = transforms.Compose([inv_normalize_array])\n",
        "    input_ten = inv_normalize(input_ten)\n",
        "    input_numpy = input_ten.numpy()\n",
        "    plt.imshow(np.transpose(input_numpy, (1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "'''Function for comparing actual images to predicted images in Tensorboard'''\n",
        "def images_to_proba(input_arr, model = res_model): #Stub function used in plot_classes_preds to \n",
        "    input_tensor = torch.clone(input_arr)\n",
        "    output = model(input_tensor)\n",
        "    _, predicted_tensor = torch.max(output, 1)\n",
        "    preds = np.squeeze(predicted_tensor.cpu().numpy())\n",
        "    return preds, [F.softmax(out, dim=0)[pred_val].item() for pred_val, out in zip(preds, output)]\n",
        "\n",
        "def plot_classes_preds(input_arr, lab, model = res_model):\n",
        "    preds, proba = images_to_proba(input_arr, model)\n",
        "    print(preds)\n",
        "    print(proba)\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    for i in range(4):\n",
        "        ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
        "        show_image(input_arr[i])\n",
        "        ax.set_title('{0}, {1:.1f}%\\n(label: {2})'.format(classes[preds[i]], proba[i]*100, classes[lab[i]]), color=('green' if preds[i]==lab[i].item() else 'red')) #\n",
        "        plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "'Model training and testing function'\n",
        "\n",
        "\n",
        "def train_model(model=res_model, optimizer=optimizer, loss_type = criterion, num_epochs = 30, mode_scheduler = scheduler, batch_size = 32, image_type='image_array', split_in_datset=False):\n",
        "    best_model_weights = copy.deepcopy(model.state_dict()) #May be changed at end of each \"for phase block\"\n",
        "    best_accuracy = 0 # May be changed at end of each \"for phase block\"\n",
        "    start = time.time()\n",
        "    writer = SummaryWriter()\n",
        "    train_size, test_size, data_loader_dict = get_loader(batch_size=batch_size, img=image_type, split_in_dataset=split_in_datset)\n",
        "    dataset_size = {'train': train_size, 'eval': test_size}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'eval']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            running_loss = 0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for batch_num, (inputs, labels) in enumerate(data_loader_dict[phase], start=1):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad() # Gradients reset to zero at beginning of both training and evaluation phase\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # print(inputs)\n",
        "                    # print(inputs.size())\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = torch.softmax(outputs, dim=1)\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "                    loss = loss_type(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() #Calculates gradients\n",
        "                        optimizer.step()\n",
        "\n",
        "                if batch_num%100==0:\n",
        "                    '''Writer functions for batch'''\n",
        "                    #writer.add_figure('Predictions vs Actual',plot_classes_preds(input_arr=inputs, lab=labels, model=model))\n",
        "                    writer.add_scalar(f'Accuracy for phase {phase} by batch number', preds.eq(labels).sum()/batch_size, batch_num)\n",
        "                    writer.add_scalar(f'Average loss for phase {phase} by batch number', loss.item(), batch_num)\n",
        "\n",
        "                running_corrects = running_corrects + preds.eq(labels).sum()\n",
        "                running_loss = running_loss + (loss.item()*inputs.size(0))\n",
        "\n",
        "            if (phase=='train') and (mode_scheduler is not None):\n",
        "                mode_scheduler.step()\n",
        "\n",
        "            '''Writer functions for epoch'''\n",
        "            epoch_loss = running_loss / dataset_size[phase]\n",
        "            print(f'Size of dataset for phase {phase}', dataset_size[phase])\n",
        "            epoch_acc = running_corrects / dataset_size[phase]\n",
        "            writer.add_scalar(f'Accuracy by epoch phase {phase}', epoch_acc, epoch)\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            writer.add_scalar(f'Average loss by epoch phase {phase}', epoch_loss, epoch)\n",
        "\n",
        "            if phase == 'eval' and epoch_acc > best_accuracy:\n",
        "                best_accuracy = epoch_acc\n",
        "                best_model_weights = copy.deepcopy(model.state_dict())\n",
        "                print(f'Best val Acc: {best_accuracy:.4f}')\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    torch.save(model.state_dict(), 'image_model.pt')\n",
        "    time_diff = time.time()-start\n",
        "    print(f'Time taken for model to run: {(time_diff//60)} minutes and {(time_diff%60):.0f} seconds')\n",
        "    return model\n",
        "\n",
        "model_tr = train_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "0L-B7TOcYmuR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c9c72559dcf4197a4a504166dddb085",
            "f20a11987fdf492fbcdd9d0e2e4eb243",
            "0365ef81a92d4c06b553bdaec8bc1ad2",
            "55a36b46ee3d4b0786d5c38b90df80f2",
            "5d33ac5ac67442d7a74095c270f98965",
            "2c72bbc646e64151910101b5f822b2a4",
            "6d08b866c2df420b9e4b5d19e115a8b4",
            "74f7165234ba4cf681cfe58403db54c1",
            "d1a675f2405e48a4bbe0c800c2bb61d1",
            "4e672e1eebba418fa6a2ad0e030f7c75",
            "8a964add036f45a08455c84aa19d5cad"
          ]
        },
        "outputId": "68b6afae-f230-42b2-84e6-570297add63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c9c72559dcf4197a4a504166dddb085"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12668\n",
            "                               image_id  \\\n",
            "0  e0ffd840-fb95-4284-b422-edbafb6848fd   \n",
            "1  7ac78bfb-73db-41e8-b6d1-c9f158995c06   \n",
            "2  6c4937e3-b1da-4631-b55f-b80cda35756b   \n",
            "3  b2d1f493-eb35-4a4a-877a-0089818f83a4   \n",
            "4  9d2f82da-3544-4182-9bcf-9ad19a2e59c1   \n",
            "\n",
            "                                      image  \\\n",
            "0  e0ffd840-fb95-4284-b422-edbafb6848fd.jpg   \n",
            "1  7ac78bfb-73db-41e8-b6d1-c9f158995c06.jpg   \n",
            "2  6c4937e3-b1da-4631-b55f-b80cda35756b.jpg   \n",
            "3  b2d1f493-eb35-4a4a-877a-0089818f83a4.jpg   \n",
            "4  9d2f82da-3544-4182-9bcf-9ad19a2e59c1.jpg   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                       image_array  \\\n",
            "0  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [2, 2, 2], [5, 5, 5], [0, 0, 0], [26, 26, 26], [31, 31, 31], [40, 40, 40], [38, 38, 38], [22, 23, 18], [29, 30, 25], [2...   \n",
            "1  [[[255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 255, 255], [255, 253, 255], [255, 253, 255], [255, 253, 255], [255, 254, 255], [255, 255, 255], [254, 255, 255], [252, 255, 255], [25...   \n",
            "2  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]...   \n",
            "3  [[[1, 1, 1], [230, 230, 230], [227, 227, 227], [228, 228, 228], [229, 229, 229], [228, 228, 228], [230, 230, 230], [228, 228, 228], [227, 227, 227], [227, 227, 227], [227, 227, 227], [227, 227, 227], [227, 227, 227], [227, 227, 227], [227, 227, 227], [227, 227, 227], [229, 227, 230], [229, 227, 230], [229, 227, 230], [229, 227, 230], [229, 227, 230], [229, 227, 230], [229, 227, 230], [229, 227...   \n",
            "4  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [1, 1, 1], [1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [0, 4, 0], [0, 4, 0], [0, 4, 0], [0, 4, 0]...   \n",
            "\n",
            "     image_shape mode  \\\n",
            "0  (224, 224, 3)  RGB   \n",
            "1  (224, 224, 3)  RGB   \n",
            "2  (224, 224, 3)  RGB   \n",
            "3  (224, 224, 3)  RGB   \n",
            "4  (224, 224, 3)  RGB   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                        edge_array  \\\n",
            "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002192223507352735, 0.004999038738816455, 0.010961117536763675, 0.008084520834544431, 0.06880229827263823, 0.09550650100358865, 0.04077771439820671, 0.012782749814122831, 0.058670377687493024, 0.003282576655060409, 0.07852935056666974, 0.09415848787092526, 0.06772516093792369,...   \n",
            "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003967562284398744, 0.003967562284398744, 0.0, 0.001983781142199372, 0.003967562284398744, 0.0013945254912106477, 0.001767766952966331, 3.925231146709437e-17, 0.0009936162637094197, 0.0014023674441062245, 0.0008569990917679379, 0.04523920488377258, 0.47696714040212007, 0.4612804116295152, 0.006406204226775436, 0.061...   \n",
            "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
            "3  [[0.634316756061494, 0.6260009286173286, 0.00620054443170277, 0.006576670522058231, 0.0009803921568627694, 0.002772967769359, 3.925231146709437e-17, 0.008318903308077098, 0.002772967769359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0017783042304899423, 0.0017783042304899423, 1.9626155733547187e-17, 1.9626155733547187e-17, 1.9626155733547187e-17, 1.9626155733547187e-17, 1.9626155733547187e-17, 1.96261557...   \n",
            "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0027729677693590095, 0.0027729677693590095, 0.0027729677693590095, 0.0027729677693590095, 0.0, 0.0023891890300797233, 0.0023891890300797233, 3.066586833366748e-19, 3.066586833366748e-19, 3.066586833366748e-19, 3.066586833366748e-19, 3.066586833366748e-19, 3.0665...   \n",
            "\n",
            "                                     id  \\\n",
            "0  394a4a28-1c8e-4fd7-8342-22803a2b8d8c   \n",
            "1  3c852f17-a4d6-4677-a6f4-e3145987ce3f   \n",
            "2  f217055b-7213-4d89-8e9f-f118d0ff7081   \n",
            "3  a6000ba2-701e-4223-bfc3-27293e6a5e08   \n",
            "4  6fe03985-8de8-4f50-ace3-f706e9b5e272   \n",
            "\n",
            "                                                                               bucket_link  \\\n",
            "0  https://aicore-product-images.s3.amazonaws.com/e0ffd840-fb95-4284-b422-edbafb6848fd.jpg   \n",
            "1  https://aicore-product-images.s3.amazonaws.com/7ac78bfb-73db-41e8-b6d1-c9f158995c06.jpg   \n",
            "2  https://aicore-product-images.s3.amazonaws.com/6c4937e3-b1da-4631-b55f-b80cda35756b.jpg   \n",
            "3  https://aicore-product-images.s3.amazonaws.com/b2d1f493-eb35-4a4a-877a-0089818f83a4.jpg   \n",
            "4  https://aicore-product-images.s3.amazonaws.com/9d2f82da-3544-4182-9bcf-9ad19a2e59c1.jpg   \n",
            "\n",
            "          image_ref create_time  \n",
            "0  ZHMAAOSwnXxiEn6d  2022-02-27  \n",
            "1  Dd0AAOSw~QRiGQAn  2022-02-28  \n",
            "2  qKUAAOSwwCNfw-sL  2022-02-26  \n",
            "3  z0oAAOSwIRhiGRG1  2022-02-26  \n",
            "4  4KwAAOSwFKFfkBJK  2022-02-27  \n",
            "0     1430\n",
            "11    1149\n",
            "10    1134\n",
            "7     1079\n",
            "3     1009\n",
            "2      905\n",
            "9      903\n",
            "6      897\n",
            "8      855\n",
            "12     827\n",
            "4      786\n",
            "5      763\n",
            "1      675\n",
            "Name: major_category_encoded, dtype: int64\n",
            "Home & Garden                        1430\n",
            "Office Furniture & Equipment         1149\n",
            "Computers & Software                 1134\n",
            "Health & Beauty                      1079\n",
            "Music, Films, Books & Games          1009\n",
            "DIY Tools & Materials                 905\n",
            "Appliances                            903\n",
            "Other Goods                           897\n",
            "Sports, Leisure & Travel              855\n",
            "Video Games & Consoles                827\n",
            "Phones, Mobile Phones & Telecoms      786\n",
            "Clothes, Footwear & Accessories       763\n",
            "Baby & Kids Stuff                     675\n",
            "Name: major_category, dtype: int64\n",
            "Total observations in remaining dataset:  12412\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0748\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0822\n",
            "Best val Acc: 0.0822\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0746\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0846\n",
            "Best val Acc: 0.0846\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0768\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0826\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0768\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0805\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0760\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0838\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0765\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0838\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0770\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0830\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0792\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0834\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0757\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0822\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0762\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0814\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0747\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0826\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0778\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0826\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0776\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0838\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0742\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0846\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0768\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0834\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0758\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0842\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0749\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0805\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0764\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0763\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0774\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0834\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0767\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0810\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0760\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0830\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0769\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0838\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0774\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0810\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0763\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0783\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0834\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0748\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0838\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0755\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0826\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0759\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0764\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0810\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0751\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0810\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0763\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0830\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0773\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0830\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0772\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0765\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0814\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0759\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0822\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0777\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0814\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0762\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0822\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0757\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0842\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0750\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0826\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0773\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0822\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0766\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0834\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0760\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0805\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0781\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0810\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0769\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0826\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0764\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0746\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0818\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0771\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0846\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0772\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0822\n",
            "Size of dataset for phase train 9929\n",
            "train Loss: 2.5648 Acc: 0.0792\n",
            "Size of dataset for phase eval 2483\n",
            "eval Loss: 2.5646 Acc: 0.0834\n",
            "Time taken for model to run: 42.0 minutes and 49 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=runs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "vchUunJ_PsSs",
        "outputId": "7b96638c-e285-4a09-872e-78cfde811a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 9393), started 0:04:35 ago. (Use '!kill 9393' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "print(os.getcwd())\n",
        "print(os.path.relpath(Path.cwd(), Path.home()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQjQP2qCPrhQ",
        "outputId": "79498427-fd07-4dc7-bd32-985b69c6fd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "../content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook.list()\n",
        "notebook.display(port = 6008, height = 1000)"
      ],
      "metadata": {
        "id": "7OCVdW8lZzSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Uui-jFTzZ13z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QhW_fOijbfUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0cjRBzoneACK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('facebook_mkt')"
      ],
      "metadata": {
        "id": "U4aWlmnMeGs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZCCCdbBgTq6",
        "outputId": "312f40f7-62dd-46e1-ca8c-5c00874d72c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aws_tool.py',\n",
              " 'discarded_temp_code.py',\n",
              " 'sketch_df_greyscale.xlsx',\n",
              " 'sketch_df_greyscale_flattened.xlsx',\n",
              " 'sketch_df_greyscale_flattened_2.xlsx',\n",
              " 'sketch_df_greyscale_unnorm.xlsx',\n",
              " 'svm_sketchpad.ipynb',\n",
              " 'mnist.ipynb',\n",
              " 'detailed_prod_clean.ipynb',\n",
              " 'README.md',\n",
              " 'words.ipynb',\n",
              " 'linear_regression.ipynb',\n",
              " 'dum.py',\n",
              " 'pytorch_scratch_classification.py',\n",
              " 'Dockerfile',\n",
              " '.gitignore',\n",
              " 'sketch_transfer.ipynb',\n",
              " 'svm_image_regression.py',\n",
              " 'pytorch_image_transfer_classification.py',\n",
              " '.DS_Store',\n",
              " '.git',\n",
              " 'images',\n",
              " '__pycache__',\n",
              " 'logs',\n",
              " 'data_files',\n",
              " 'runs',\n",
              " 'clean_tabular.py',\n",
              " 'clean_images.py']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTQDLzjagbQn",
        "outputId": "1c6608db-0c7f-49d4-b73c-36000b76e60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 32.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchbearer\n",
            "  Downloading torchbearer-0.5.3-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 28.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchbearer) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->torchbearer) (4.2.0)\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pytorch_image_transfer_classification.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTGb20pan_6S",
        "outputId": "383ee42a-bde2-40fe-d018-cc844abce54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 245MB/s]\n",
            "{0: 'Home & Garden ', 1: 'Baby & Kids Stuff ', 2: 'DIY Tools & Materials ', 3: 'Music, Films, Books & Games ', 4: 'Phones, Mobile Phones & Telecoms ', 5: 'Clothes, Footwear & Accessories ', 6: 'Other Goods ', 7: 'Health & Beauty ', 8: 'Sports, Leisure & Travel ', 9: 'Appliances ', 10: 'Computers & Software ', 11: 'Office Furniture & Equipment ', 12: 'Video Games & Consoles '}\n",
            "{'Home & Garden ': 0, 'Baby & Kids Stuff ': 1, 'DIY Tools & Materials ': 2, 'Music, Films, Books & Games ': 3, 'Phones, Mobile Phones & Telecoms ': 4, 'Clothes, Footwear & Accessories ': 5, 'Other Goods ': 6, 'Health & Beauty ': 7, 'Sports, Leisure & Travel ': 8, 'Appliances ': 9, 'Computers & Software ': 10, 'Office Furniture & Equipment ': 11, 'Video Games & Consoles ': 12}\n",
            "12667\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xFgGLRY_sJS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s6IbhQAqsM8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Path.home())\n",
        "print(Path.cwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8we1LKroW8s",
        "outputId": "75f06296-bcbe-46cc-ad7f-e37c5d2b2873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content/drive/MyDrive/facebook_mkt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.relpath(Path.cwd(), Path.home())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Al6EjBMfqWQQ",
        "outputId": "5382439a-8291-4537-8969-95f4e4bdeea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../content/drive/MyDrive/facebook_mkt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.abspath(Path.cwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xYCFvNc8rAUs",
        "outputId": "ca79e79a-d166-42ad-ba64-a8894ac6b270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/facebook_mkt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8xuI_JGgrPwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}