{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajj8866/facebook_mkt/blob/main/fb_gdrive_trainV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYaDUsJATLoU",
        "outputId": "4679ed84-fe80-4b5d-bf80-436547a483b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "Current working directory after mounting:  /content\n",
            "Home directory after mounting:  /root\n",
            "Absolute path to facebook_mkt directory:  /content/drive/MyDrive/facebook_mkt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files, drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "drive.mount('/content/drive/') #Mounting drive folder to contain the folder facebook_mkt\n",
        "print('Current working directory after mounting: ',os.getcwd())\n",
        "print('Home directory after mounting: ', Path.home())\n",
        "os.chdir(Path(Path.cwd(), 'drive','MyDrive', 'facebook_mkt'))\n",
        "print('Absolute path to facebook_mkt directory: ', os.path.abspath(Path.cwd()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtAv-mBATxSF",
        "outputId": "e5f5043b-fd98-41b7-d37a-03f65a165e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: missing destination file operand after 'images.zip'\n",
            "Try 'cp --help' for more information.\n",
            "replace __MACOSX/._images? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/images/._e0ffd840-fb95-4284-b422-edbafb6848fd.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!cp 'images.zip'\n",
        "!unzip -q images.zip\n",
        "!rm images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWAiKLR-TxZx",
        "outputId": "57c00e24-061e-484b-a45c-93be2726c994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchbearer\n",
            "  Downloading torchbearer-0.5.3-py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchbearer) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchbearer) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->torchbearer) (4.2.0)\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.0.3\n",
            "/content/drive/MyDrive/facebook_mkt\n",
            "['images.zip', 'torch_fb_run.ipynb', 'data_files', '__MACOSX']\n"
          ]
        }
      ],
      "source": [
        "!pip install torchbearer\n",
        "!pip install XlsxWriter\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBXIlvQYTxgZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e4444210fc4d4cbfb819ca4f16a0bdfc",
            "f743c593756e4d51b3ee6e82bd558589",
            "a489738492ad4bbaaae5798011e25782",
            "40196bd6cf234b149bd44efdb7242ea6",
            "64e1a5b57c394222ac0e9281e0592140",
            "4887e3570d4e41c490a741c7d60bb913",
            "5545ab64571045d2bef79bd60efb1617",
            "de4960418c394051a701c814c4c9e810",
            "0deb440a7db545ea856a768c8d90695e",
            "aa71764852604128930c163bc6265010",
            "3663899bc59d4577b5a32e7feb5bf4cc"
          ]
        },
        "id": "BhNSGslsTxmC",
        "outputId": "f0a64ade-36db-4584-9381-0601cbc61842"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4444210fc4d4cbfb819ca4f16a0bdfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m \u001b[0mmodel_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_type, num_epochs, mode_scheduler, batch_size, image_type, split_in_datset)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_in_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_in_datset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36mget_loader\u001b[0;34m(img, batch_size, split_in_dataset, train_prop)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mimage_datsets\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mtrain_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage_datsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transformer, X, y, img_dir, img_size, train_proportion, is_test)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mmerge_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMergedData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_encoded$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor_map_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprod_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor_map_decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprod_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Products'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36mtotal_clean\u001b[0;34m(self, normalize, mode, size)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtotal_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_clean_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_clean_sk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-87a3bcf6a6d2>\u001b[0m in \u001b[0;36mimg_clean_pil\u001b[0;34m(self, size, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimg_clean_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mimage_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(.*)\\.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;31m# os.chdir(Path(Path.cwd(), 'images'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/facebook_mkt/images'"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sqlalchemy import true\n",
        "import os\n",
        "import numpy as np\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from itertools import product\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import json \n",
        "import torchvision.transforms as transforms\n",
        "import re\n",
        "from PIL import Image\n",
        "import multiprocessing\n",
        "import torchvision\n",
        "from skimage import io\n",
        "from skimage import img_as_float\n",
        "from skimage.filters import sobel\n",
        "from skimage.color import rgb2gray\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchbearer import Trial\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision.transforms import Normalize, ToPILImage, ToTensor\n",
        "from torchbearer.callbacks import TensorBoard\n",
        "from torch.nn import Module\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "from torchvision import models, datasets\n",
        "import copy\n",
        "import time\n",
        "from tensorboard import notebook\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "class CleanData:\n",
        "    def __init__(self, tab_names = ['Products']) -> None:\n",
        "        self.tab_names = tab_names\n",
        "        maj_unique_cats = ['Home & Garden ', 'Baby & Kids Stuff ', 'DIY Tools & Materials ', 'Music, Films, Books & Games ', 'Phones, Mobile Phones & Telecoms ', 'Clothes, Footwear & Accessories ', 'Other Goods ', 'Health & Beauty ', 'Sports, Leisure & Travel ', 'Appliances ', 'Computers & Software ','Office Furniture & Equipment ', 'Video Games & Consoles ']\n",
        "        self.major_map_decoder = dict(enumerate(maj_unique_cats))\n",
        "        self.major_map_encoder = {val: key for key, val in self.major_map_decoder.items()}\n",
        "        if 'data_files' not in os.listdir():\n",
        "            os.mkdir(Path(Path.cwd(), 'data_files'))\n",
        "        self.table_dict = {}\n",
        "        for table in tab_names:\n",
        "            self.table_dict[table] = pd.read_json(Path(Path.cwd(),'data_files', table+'.json'))\n",
        "            self.table_dict[table].dropna(inplace = True)\n",
        "            if 'price' in self.table_dict[table].columns:\n",
        "                self.table_dict[table]['price'] = self.table_dict[table][self.table_dict[table]['price'] != 'N/A'.strip()]['price']\n",
        "                self.table_dict[table]['price'] = self.table_dict[table]['price'].str.replace(',', '').str.strip('£').str.strip(' ').astype(np.float32)\n",
        "                self.table_dict[table] = self.table_dict[table][np.round(self.table_dict[table]['price']) != 0]\n",
        "            if 'category' in self.table_dict[table].columns:\n",
        "                self.expand_category(df=table)\n",
        "\n",
        "    \n",
        "    def try_merge(self, df_list):\n",
        "        '''\n",
        "        Combines dataframes passed in into a single dataframe\n",
        "\n",
        "        Parameters:\n",
        "        df_list: Must contain dataframes within self.table_dict passed in as a list\n",
        "        '''\n",
        "        if isinstance(self.tab_names, str):\n",
        "            print('Method not valid when class instantiated with tab_names as type string')\n",
        "        else:\n",
        "            self.new_df = pd.DataFrame(columns = self.table_dict[df_list[0]].columns)\n",
        "            for i in df_list:\n",
        "                self.new_df = pd.concat([self.new_df, self.table_dict[i]], axis=0)\n",
        "        self.table_dict['combined'] = self.new_df\n",
        "        self.table_dict['combined'].dropna(inplace=True)\n",
        "        return self.table_dict['combined']\n",
        "    \n",
        "    def get_na_vals(self, df):\n",
        "        print(f'The following NA values exist if dataframe {df}')\n",
        "        return self.table_dict[df][self.table_dict[df].isna().any(axis=1)]\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        if isinstance(self.tab_names, str):\n",
        "            print(self.df.columns)\n",
        "            print('\\nTable Name: ', self.tab_names, 'With columns:')\n",
        "            return ' | '.join(self.df.columns)\n",
        "        else:\n",
        "            print('\\n')\n",
        "            print('Total of ', f'{len(self.table_dict)} tables')\n",
        "            return '\\n'.join([f'Table Name: {i}: \\n' f'Columns | {\" | \".join(j.columns)} \\n' for i, j in self.table_dict.items()])\n",
        "\n",
        "    def to_excel(self):\n",
        "        for i, j in self.table_dict.items():\n",
        "            ex_writer = pd.ExcelWriter(f'data_files/{i}.xlsx', engine='xlsxwriter')\n",
        "            with ex_writer as writer:\n",
        "                j.to_excel(writer, sheet_name=i)\n",
        "    \n",
        "    def cat_set(self, df = 'Products',cat_col = 'major_category'):\n",
        "        return self.table_dict[df][cat_col].nunique()\n",
        "    \n",
        "    def expand_category(self, df = 'Products'):\n",
        "        self.major_encoder = LabelEncoder()\n",
        "        self.minor_encoder = LabelEncoder()\n",
        "        self.table_dict[df]['major_category'] = self.table_dict[df]['category'].str.split('/').apply(lambda i: i[0])\n",
        "        self.table_dict[df]['minor_category'] = self.table_dict[df]['category'].str.split('/').apply(lambda i: i[1])\n",
        "        self.table_dict[df] = self.table_dict[df][self.table_dict[df]['major_category'] != 'N'.strip()]\n",
        "        self.table_dict[df]['major_category_encoded'] = self.table_dict[df]['major_category'].map(self.major_map_encoder)\n",
        "        self.table_dict[df]['minor_category_encoded'] = self.minor_encoder.fit_transform(self.table_dict[df]['minor_category'])\n",
        "        return self.table_dict[df]\n",
        "    \n",
        "    def inverse_transform(self, input_array, major_minor = 'minor'):\n",
        "        category_dict = {'major': self.major_encoder, 'minor': self.minor_encoder}\n",
        "        try:\n",
        "            return category_dict[major_minor].inverse_transform(input_array)\n",
        "        except TypeError:\n",
        "            return category_dict[major_minor].inverse_transform(input_array.numpy())\n",
        "    \n",
        "    \n",
        "    def sum_by_cat(self, df= 'Products', quant = 0.95):\n",
        "        data = self.expand_category(df)\n",
        "        major = data.groupby('major_category')['price'].describe()\n",
        "        print('Price Statistics Grouped by Major Category')\n",
        "        print(major)\n",
        "        major_cat_list = major.index.tolist()\n",
        "        #sns.boxplot(data=data, x = 'major_category', y = 'price')\n",
        "        products_df = data.loc[:, ['major_category', 'minor_category', 'price']]\n",
        "        for i in major_cat_list:\n",
        "            prod_plot = products_df.loc[products_df['major_category'] == i]\n",
        "            # print(prod_plot['price'].quantile([quant]))\n",
        "            # print(type(prod_plot['price'].quantile([quant][0])))\n",
        "            # print('Number of observations with price more than the 99th quantile: ', len(prod_plot[prod_plot['price'] > prod_plot['price'].quantile([quant][0])]))\n",
        "            # sns.boxplot(data=prod_plot, x='major_category', y='price')\n",
        "            # plt.show()\n",
        "            sns.boxplot(data=prod_plot[prod_plot['price']<prod_plot['price'].quantile([quant][0])], x = 'major_category', y = 'price')\n",
        "            plt.show()\n",
        "\n",
        "    def trim_data(self, df= 'Products', quant = 0.95):\n",
        "        self.table_dict[df] = self.table_dict[df][self.table_dict[df]['price'] > self.table_dict[df]['price'].quantile([quant])]\n",
        "        return self.table_dict[df]\n",
        "\n",
        "    @classmethod\n",
        "    def allTables(cls):\n",
        "        json_list = []\n",
        "        json_regex = re.compile(r'(.*).json$')\n",
        "        for i in os.listdir(Path(Path.cwd(), 'data_files')):\n",
        "            if re.search(json_regex, i) is not None:\n",
        "                json_list.append(re.search(json_regex, i).group(1))\n",
        "        print(json_list)\n",
        "        return cls(tab_names = json_list)\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "class CleanImages(CleanData):\n",
        "    def __init__(self, tab_names=['Images']) -> None:\n",
        "        super().__init__(tab_names)\n",
        "        self.df = self.table_dict[tab_names[0]].copy()\n",
        "        self.csv_df = None\n",
        "\n",
        "    def img_clean_pil(self, size = 512, mode = 'RGB'):\n",
        "        image_re = re.compile(r'(.*)\\.jpg')\n",
        "        os.chdir(Path(Path.cwd(), 'images'))\n",
        "        # os.chdir(Path(Path.cwd(), 'images'))\n",
        "        t = 0\n",
        "        for i in os.listdir():\n",
        "            if re.findall(image_re, i) != []:\n",
        "                try:\n",
        "                    temp_image = Image.open(i)\n",
        "                    black_back = Image.new(size=(size, size), mode=temp_image.mode) #, mode=mode\n",
        "                    curr_size = temp_image.size\n",
        "                    max_dim = max(temp_image.size)\n",
        "                    scale_fact = size / max_dim\n",
        "                    resized_image_dim = (int(scale_fact*curr_size[0]), int(scale_fact*curr_size[1]))\n",
        "                    updated_image = temp_image.resize(resized_image_dim)\n",
        "                    black_back.paste(updated_image, ((size- resized_image_dim[0])//2, (size- resized_image_dim[1])//2))\n",
        "                    black_back = black_back.convert(mode)\n",
        "                    t += 1\n",
        "                    black_back.save(i)\n",
        "                except Exception:\n",
        "                    print(i)\n",
        "                    with open('invalid_file.json', 'w') as wrong_form:\n",
        "                        json.dump(i, wrong_form)\n",
        "                    os.remove(i)\n",
        "                    pass\n",
        "        print(t)\n",
        "        os.chdir(Path(Path.cwd().parents[0]))\n",
        "\n",
        "    def img_clean_sk(self, normalize = False):\n",
        "        image_re = re.compile(r'(.*)\\.jpg')\n",
        "        img = []\n",
        "        img_dim_list = []\n",
        "        img_id = []\n",
        "        image_array = []\n",
        "        img_channels = []\n",
        "        img_num_features = []\n",
        "        img_mode = []\n",
        "        os.chdir(Path(Path.cwd(), 'images'))\n",
        "        for im in os.listdir():\n",
        "            if re.findall(image_re, im) != []:\n",
        "                img.append(im)\n",
        "                image = io.imread(im)\n",
        "                if normalize == True:\n",
        "                    image = img_as_float(image)\n",
        "                img_id.append(re.search(image_re, im).group(1))\n",
        "                image_array.append(image)\n",
        "                img_dim_list.append(image.shape)\n",
        "                if len(image.shape) == 3:\n",
        "                    img_num_features.append(image.shape[2])\n",
        "                else:\n",
        "                    img_num_features.append(1)\n",
        "                img_channels.append(len(image.shape))\n",
        "                img_mode.append(Image.open(im).mode)\n",
        "        os.chdir(Path(Path.cwd().parents[0]))\n",
        "        self.image_frame = pd.DataFrame(data={'image_id': img_id, 'image': img,'image_array': image_array,'image_shape': img_dim_list, 'mode': img_mode})\n",
        "        return self.image_frame\n",
        "    \n",
        "    def to_excel(self, df):\n",
        "        df.to_excel(Path(Path.cwd(), 'data_files','Cleaned_Images.xlsx'), sheet_name = 'images')\n",
        "\n",
        "    def merge_images(self):\n",
        "        self.df.rename({'id': 'image_id', 'product_id': 'id'}, axis=1, inplace=True)\n",
        "        self.final_df = self.image_frame.merge(self.df, on='image_id', how='inner', validate='one_to_many')\n",
        "        #print(self.final_df.head())\n",
        "        return self.final_df\n",
        "    \n",
        "    def edge_detect(self):\n",
        "        try:\n",
        "            self.image_frame['edge_array'] = self.image_frame['image_array'].copy().apply(lambda i: sobel(rgb2gray(i)))\n",
        "        except: \n",
        "            self.image_frame['edge_array'] = self.image_frame['image_array'].copy().apply(lambda i: sobel(i))\n",
        "        return self.image_frame\n",
        "\n",
        "\n",
        "    def total_clean(self, normalize=False, mode = 'RGB', size = 224):\n",
        "        self.img_clean_pil(mode=mode, size=size)\n",
        "        self.img_clean_sk(normalize=normalize)\n",
        "        self.edge_detect()\n",
        "        self.merge_images()\n",
        "        return self.final_df\n",
        "    \n",
        "    def show_random_images(self, col, size, fig_height= 15, fig_width=10):\n",
        "        grid = GridSpec(nrows = size, ncols = size)\n",
        "        fig = plt.figure(figsize=(fig_height, fig_width))\n",
        "        for i, j in product(range(size), range(size)):\n",
        "            fig.add_subplot(grid[i, j]).imshow(self.final_df[col].iloc[np.random.randint(low=0, high=len(self.final_df)-1)])\n",
        "        plt.show()\n",
        "\n",
        "    def describe_data(self, df):\n",
        "        print('\\n')\n",
        "        print('Data frame columnn information')\n",
        "        print(df.info())\n",
        "        print('\\n')\n",
        "        print('#'*20)\n",
        "        print('Dataframe statistical metrics')\n",
        "        #print(df.describe())\n",
        "        print('#'*20)\n",
        "        print('Array and shape')\n",
        "        print(df['image_shape'].unique())\n",
        "        print(df['image_shape'].value_counts())\n",
        "\n",
        "#############################################################################################\n",
        "\n",
        "class MergedData:\n",
        "    def __init__(self):\n",
        "        img_class = CleanImages()\n",
        "        prod_class = CleanData(tab_names=['Products'])\n",
        "        self.major_map_encoder = prod_class.major_map_encoder\n",
        "        self.major_map_decoder = prod_class.major_map_decoder\n",
        "        self.prod_frame = prod_class.table_dict['Products'].copy()\n",
        "        self.img_df = img_class.total_clean()\n",
        "        self.merged_frame = self.img_df.merge(self.prod_frame, left_on='id', right_on='id')\n",
        "    \n",
        "    def to_pickle(self):\n",
        "        self.merged_frame.to_pickle(Path(Path.cwd(), 'merged_data.pkl'))\n",
        "    \n",
        "    def get_val_counts(self):\n",
        "        return {'products': self.prod_frame, 'images': self.img_df, 'all': self.merged_frame}\n",
        "      \n",
        "#############################################################################################\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, transformer = transforms.Compose([ToTensor()]), X = 'image_array', y = 'major_category_encoded', img_dir = Path(Path.cwd(), 'images'), img_size=224, train_proportion = 0.8, is_test = False):\n",
        "        '''\n",
        "        X: Can be either 'image' if dataset to be instantiated using image object or 'image_array' if dataset to be instantiated using numpy array \n",
        "        y: Can be either 'major_category_encoded' or 'minor_category_encoded'\n",
        "        '''\n",
        "        self.img_inp_type = X\n",
        "        self.transformer = transformer\n",
        "        self.img_dir = img_dir\n",
        "        self.img_size = img_size\n",
        "        merge_class = MergedData()\n",
        "        merged_df = merge_class.merged_frame\n",
        "        filtered_df = merged_df.loc[:, ['image_id', X, re.sub(re.compile('_encoded$'), '', y), y]].copy()\n",
        "        filtered_df.dropna(inplace=True)\n",
        "        print(filtered_df[y].value_counts())\n",
        "        print(filtered_df[re.sub(re.compile('_encoded$'), '', y)].value_counts())\n",
        "        train_end = int(len(filtered_df)*train_proportion)\n",
        "        if is_test == False:\n",
        "            filtered_df = filtered_df.iloc[:train_end]\n",
        "        elif is_test == True:\n",
        "            filtered_df = filtered_df.iloc[train_end:]\n",
        "        else:\n",
        "            pass\n",
        "        self.dataset_size = len(filtered_df)\n",
        "        self.all_data = filtered_df\n",
        "        print('Total observations in remaining dataset: ', len(filtered_df))\n",
        "        self.y = torch.tensor(filtered_df[y].values)\n",
        "        self.X = filtered_df[X].values\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        if self.img_inp_type == 'image':\n",
        "            try:\n",
        "                self.X[idx] =  Image.open(os.path.join(self.img_dir, self.X[idx]))\n",
        "                if self.transformer is not None:\n",
        "                    self.X[idx] = self.transformer(self.X[idx])\n",
        "            except TypeError:\n",
        "                self.X[idx] = self.X[idx]\n",
        "        elif self.img_inp_type == 'image_array':\n",
        "            try:\n",
        "                # self.X[idx] = torch.from_numpy(np.transpose(self.X[idx], (2,1,0)))\n",
        "                if self.transformer is not None:\n",
        "                    self.X[idx] = self.transformer(self.X[idx])\n",
        "            except TypeError:\n",
        "                self.X[idx] = self.X[idx]\n",
        "        else:\n",
        "            self.X[idx] = self.X[idx]        \n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "#############################################################################################\n",
        "\n",
        "pd.set_option('display.max_colwidth', 400)\n",
        "pd.set_option('display.max_columns', 15)\n",
        "pd.set_option('display.max_rows', 40)\n",
        "plt.rc('axes', titlesize=12)\n",
        "\n",
        "res_model = models.resnet50(pretrained=True)\n",
        "for param in res_model.parameters():\n",
        "    param.requires_grad = False\n",
        "res_model.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=512, bias=True), nn.ReLU(inplace=True), nn.Dropout(p=0.2), nn.Linear(in_features=512, out_features=64), nn.Linear(in_features=64, out_features=13))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "res_model.to(device)\n",
        "\n",
        "opt = optim.SGD\n",
        "optimizer =  opt(res_model.parameters(), lr=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[5, 10, 15, 20, 25, 30], gamma=0.3) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def get_loader(img = 'image_array',batch_size=35, split_in_dataset = False, train_prop = 0.8):\n",
        "    train_transformer = transforms.Compose([transforms.RandomRotation(40), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    test_transformer = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    if split_in_dataset == True:\n",
        "        train_dataset = Dataset(transformer=train_transformer, X=img, img_size=224, is_test=False, train_proportion=train_prop)\n",
        "        test_dataset = Dataset(transformer=test_transformer, X=img, img_size=224, is_test=True, train_proportion=train_prop)\n",
        "        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "        test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n",
        "        data_loader_dict = {'train': train_loader, 'eval': test_loader}\n",
        "        return train_dataset.dataset_size, test_dataset.dataset_size, data_loader_dict\n",
        "    else:\n",
        "        image_datsets= Dataset(transformer=test_transformer, X = img, img_size=224, is_test=None)\n",
        "        train_end = int(train_prop*image_datsets.dataset_size)\n",
        "        train_dataset, test_dataset = random_split(image_datsets, lengths=[len(image_datsets.all_data.iloc[:train_end]), len(image_datsets.all_data.iloc[train_end:])])\n",
        "        dataset_dict = {'train': train_dataset, 'eval': test_dataset}\n",
        "        data_loader_dict = {i: DataLoader(dataset_dict[i], batch_size=batch_size, shuffle=True) for i in ['train', 'eval']}\n",
        "        return len(image_datsets.all_data.iloc[:train_end]), len(image_datsets.all_data.iloc[train_end:]), data_loader_dict\n",
        "    \n",
        "prod_dum = CleanData()\n",
        "class_dict = prod_dum.major_map_encoder.keys()\n",
        "classes = list(class_dict)\n",
        "class_values = prod_dum.major_map_encoder.values()\n",
        "class_encoder = prod_dum.major_map_encoder\n",
        "\n",
        "\n",
        "'''Tensorboard Function for Showing Images'''\n",
        "def show_image(input_ten_orig):\n",
        "    input_ten = torch.clone(input_ten_orig)\n",
        "    inv_normalize_array = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255])\n",
        "    inv_normalize = transforms.Compose([inv_normalize_array])\n",
        "    input_ten = inv_normalize(input_ten)\n",
        "    input_numpy = input_ten.numpy()\n",
        "    plt.imshow(np.transpose(input_numpy, (1, 2, 0)))\n",
        "    # plt.show()\n",
        "\n",
        "'''Function for comparing actual images to predicted images in Tensorboard'''\n",
        "def images_to_proba(input_arr, model = res_model): #Stub function used in plot_classes_preds to \n",
        "    input_tensor = torch.clone(input_arr)\n",
        "    output = model(input_tensor)\n",
        "    _, predicted_tensor = torch.max(output, 1)\n",
        "    preds = np.squeeze(predicted_tensor.cpu().numpy())\n",
        "    return preds, [F.softmax(out, dim=0)[pred_val].item() for pred_val, out in zip(preds, output)]\n",
        "\n",
        "def plot_classes_preds(input_arr, lab, model = res_model):\n",
        "    preds, proba = images_to_proba(input_arr, model)\n",
        "    print(preds)\n",
        "    print(proba)\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    for i in range(4):\n",
        "        ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
        "        show_image(input_arr[i])\n",
        "        ax.set_title('{0}, {1:.1f}%\\n(label: {2})'.format(classes[preds[i]], proba[i]*100, classes[lab[i]]), color=('green' if preds[i]==lab[i].item() else 'red')) #\n",
        "        plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "'Model training and testing function'\n",
        "\n",
        "\n",
        "def train_model(model=res_model, optimizer=optimizer, loss_type = criterion, num_epochs = 30, mode_scheduler = scheduler, batch_size = 32, image_type='image_array', split_in_datset=False):\n",
        "    best_model_weights = copy.deepcopy(model.state_dict()) #May be changed at end of each \"for phase block\"\n",
        "    best_accuracy = 0 # May be changed at end of each \"for phase block\"\n",
        "    start = time.time()\n",
        "    writer = SummaryWriter()\n",
        "    train_size, test_size, data_loader_dict = get_loader(batch_size=batch_size, img=image_type, split_in_dataset=split_in_datset)\n",
        "    dataset_size = {'train': train_size, 'eval': test_size}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in ['train', 'eval']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            running_loss = 0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for batch_num, (inputs, labels) in enumerate(data_loader_dict[phase], start=1):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad() # Gradients reset to zero at beginning of both training and evaluation phase\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # print(inputs)\n",
        "                    # print(inputs.size())\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = torch.softmax(outputs, dim=1)\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "                    loss = loss_type(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() #Calculates gradients\n",
        "                        optimizer.step()\n",
        "\n",
        "                if batch_num%100==0:\n",
        "                    '''Writer functions for batch'''\n",
        "                    #writer.add_figure('Predictions vs Actual',plot_classes_preds(input_arr=inputs, lab=labels, model=model))\n",
        "                    writer.add_scalar(f'Accuracy for phase {phase} by batch number', preds.eq(labels).sum()/batch_size, batch_num)\n",
        "                    writer.add_scalar(f'Average loss for phase {phase} by batch number', loss.item(), batch_num)\n",
        "\n",
        "                running_corrects = running_corrects + preds.eq(labels).sum()\n",
        "                running_loss = running_loss + (loss.item()*inputs.size(0))\n",
        "\n",
        "            if (phase=='train') and (mode_scheduler is not None):\n",
        "                mode_scheduler.step()\n",
        "\n",
        "            '''Writer functions for epoch'''\n",
        "            epoch_loss = running_loss / dataset_size[phase]\n",
        "            print(f'Size of dataset for phase {phase}', dataset_size[phase])\n",
        "            epoch_acc = running_corrects / dataset_size[phase]\n",
        "            writer.add_scalar(f'Accuracy by epoch phase {phase}', epoch_acc, epoch)\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            writer.add_scalar(f'Average loss by epoch phase {phase}', epoch_loss, epoch)\n",
        "\n",
        "            if phase == 'eval' and epoch_acc > best_accuracy:\n",
        "                best_accuracy = epoch_acc\n",
        "                best_model_weights = copy.deepcopy(model.state_dict())\n",
        "                print(f'Best val Acc: {best_accuracy:.4f}')\n",
        "\n",
        "\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    torch.save(model.state_dict(), 'image_model.pt')\n",
        "    time_diff = time.time()-start\n",
        "    print(f'Time taken for model to run: {(time_diff//60)} minutes and {(time_diff%60):.0f} seconds')\n",
        "    return model\n",
        "\n",
        "model_tr = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSp8f_B7dsOi"
      },
      "outputs": [],
      "source": [
        "from sql"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNazfCiuOoCNHUzgZyn1eLl",
      "include_colab_link": true,
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0deb440a7db545ea856a768c8d90695e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3663899bc59d4577b5a32e7feb5bf4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40196bd6cf234b149bd44efdb7242ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa71764852604128930c163bc6265010",
            "placeholder": "​",
            "style": "IPY_MODEL_3663899bc59d4577b5a32e7feb5bf4cc",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 83.4MB/s]"
          }
        },
        "4887e3570d4e41c490a741c7d60bb913": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5545ab64571045d2bef79bd60efb1617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64e1a5b57c394222ac0e9281e0592140": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a489738492ad4bbaaae5798011e25782": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de4960418c394051a701c814c4c9e810",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0deb440a7db545ea856a768c8d90695e",
            "value": 102530333
          }
        },
        "aa71764852604128930c163bc6265010": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de4960418c394051a701c814c4c9e810": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4444210fc4d4cbfb819ca4f16a0bdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f743c593756e4d51b3ee6e82bd558589",
              "IPY_MODEL_a489738492ad4bbaaae5798011e25782",
              "IPY_MODEL_40196bd6cf234b149bd44efdb7242ea6"
            ],
            "layout": "IPY_MODEL_64e1a5b57c394222ac0e9281e0592140"
          }
        },
        "f743c593756e4d51b3ee6e82bd558589": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4887e3570d4e41c490a741c7d60bb913",
            "placeholder": "​",
            "style": "IPY_MODEL_5545ab64571045d2bef79bd60efb1617",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
