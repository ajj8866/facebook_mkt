{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/aj8/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from collections import Counter, defaultdict\n",
    "from ntpath import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import column\n",
    "import xlsxwriter\n",
    "import os\n",
    "import seaborn as sns\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import contractions \n",
    "from pathlib import Path\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.io import imread, imshow\n",
    "from skimage import io\n",
    "import json\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import rescale, resize\n",
    "from itertools import product\n",
    "from PIL import Image\n",
    "from clean_tabular import CleanData, CleanImages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "class ProductDescpMannual(CleanData):\n",
    "    def __init__(self):\n",
    "        super().__init__(tab_names=['Products'])\n",
    "        self.label_len = len(self.major_map_decoder)\n",
    "        self.product_frame = self.table_dict['Products'].copy()\n",
    "\n",
    "    def clean_prod(self, col='product_description'):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_lemmitizer = WordNetLemmatizer()\n",
    "        punct_re = re.compile(r'[^A-Za-z \\n-]') #[^A-Za-z0-9 .]\n",
    "        self.product_frame[col] = self.product_frame[col].copy().str.replace(punct_re, '')\n",
    "        all_text_ls = []\n",
    "        main_ls = []\n",
    "        for i in self.product_frame[col]:\n",
    "            dum_ls = i.split()\n",
    "            dum_ls = [i for i in dum_ls if i not in  stop_words]\n",
    "            dum_ls = [i.replace('\\n', ' ') for i in dum_ls]\n",
    "            dum_ls = [i.strip('\\-') for i in dum_ls]\n",
    "            dum_ls = [word_lemmitizer.lemmatize(i) for i in dum_ls]\n",
    "            dum_ls = [contractions.fix(j) for j in dum_ls]\n",
    "            dum_ls = [i for i in dum_ls if i.isalpha] # and len(i)>2\n",
    "            main_ls.append(' '.join(dum_ls))\n",
    "            # print(' '.join(dum_ls))\n",
    "            all_text_ls.extend(dum_ls)\n",
    "        all_text_ls = [i.lower() for i in all_text_ls]\n",
    "\n",
    "        # wrd_counter = Counter(all_text_ls)\n",
    "        # wrd_freq_dict = {wrd: count/len(wrd_counter) for wrd, count in wrd_counter.items()}\n",
    "        # wrd_drop_proba = {wrd: 1- np.sqrt(threshold/wrd_freq_dict[wrd]) for wrd in wrd_counter.keys()}\n",
    "        # main_ls = []\n",
    "        # for i in self.product_frame[col]:\n",
    "        #     dum_ls = i.split()\n",
    "\n",
    "\n",
    "        self.product_frame[col] = main_ls \n",
    "        self.product_frame[col] = self.product_frame[col].apply(lambda i: i.lower())\n",
    "        print(len(all_text_ls))\n",
    "        return all_text_ls, len(all_text_ls)\n",
    "    \n",
    "    def word_freq(self, col='product_description', num_items=100):\n",
    "        all_words = self.clean_prod(col=col)[0]\n",
    "        count_ls = Counter(all_words)\n",
    "        if num_items is not None:\n",
    "            count_dict = {i[0]: i[1] for i in count_ls.most_common(num_items)}\n",
    "        else:\n",
    "            count_dict = {i[0]: i[1] for i in count_ls.most_common()}\n",
    "        return count_dict\n",
    "\n",
    "    def get_word_set(self, col='product_description'):\n",
    "        if col=='product_name':\n",
    "            self.clean_prod_name(col=col)\n",
    "        self.clean_prod(col=col)\n",
    "        self.full_word_ls = []\n",
    "        for i in self.product_frame[col]:\n",
    "            self.full_word_ls.extend(i.split())\n",
    "        self.full_word_set = list(set(self.full_word_ls))\n",
    "        return self.full_word_ls, self.full_word_set ,len(self.full_word_set)+1\n",
    "\n",
    "    def vocab_encoder(self, col='product_description', limit_it=False, vocab_limit=30000):\n",
    "        word_ls, word_set, full_vocab_size = self.get_word_set(col)\n",
    "        if limit_it==False:\n",
    "            vocab_size = full_vocab_size\n",
    "            self.word_encoder = defaultdict(lambda: vocab_size-1)\n",
    "            current_word_encoder = {key[0]: integer for integer, key in enumerate(Counter(word_ls).most_common()[:vocab_size-1])}\n",
    "            self.word_encoder['<UNKNOWN>'] = vocab_size-1\n",
    "            self.word_encoder.update(current_word_encoder)\n",
    "            self.word_decoder = {val: key for key, val in self.word_encoder.items()}\n",
    "        else:\n",
    "            vocab_size=vocab_limit\n",
    "            self.word_encoder = defaultdict(lambda: vocab_size)\n",
    "            current_word_encoder = {key[0]: integer for integer, key in enumerate(Counter(word_ls).most_common()[:vocab_size])}\n",
    "            self.word_encoder['<UNKNOWN>'] = vocab_size\n",
    "            self.word_encoder.update(current_word_encoder)\n",
    "            self.word_decoder = {val: key for key, val in self.word_encoder.items()}\n",
    "        \n",
    "        # self.word_encoder = {val: key for key, val in self.word_decoder.items()}\n",
    "        return self.word_encoder, self.word_decoder, self.product_frame\n",
    "\n",
    "    \n",
    "    def dataloader_preprocess(self, limit_it=True, vocab_lim=10000, context_size=5):\n",
    "        product_encoder, product_decoder, DF = self.vocab_encoder(limit_it=limit_it, vocab_limit=vocab_lim)\n",
    "        df = DF.copy()\n",
    "        init_ls = []\n",
    "        for i in range(len(df)):\n",
    "            prod_descript = df['product_description'].iloc[i].split()\n",
    "            init_ls.append([\n",
    "                [\n",
    "                    list(np.array([[prod_descript[i - j], prod_descript[i+j]] for j in range(1, context_size+1)]).flatten()),\n",
    "                    prod_descript[i]\n",
    "                ]\n",
    "                for i in range(context_size, len(prod_descript)-context_size)\n",
    "            ])\n",
    "        df['original_context_target'] = init_ls\n",
    "        df = df.explode('original_context_target').reset_index()\n",
    "        for idx, val in enumerate(df['original_context_target']):\n",
    "            if type(val) is not list:\n",
    "                df.drop(idx, axis=0, inplace=True)\n",
    "        \n",
    "        def stub_coder(ls, code_type='encoded'):\n",
    "            code_dict = {'encoded': product_encoder, 'decoded': product_decoder}\n",
    "            new_ls = []\n",
    "            for i in ls.copy():\n",
    "                new_ls.append(code_dict[code_type][i])\n",
    "            return new_ls\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(df['original_context_target'].tolist())], axis=1)\n",
    "        df.rename(columns={0: 'context', 1: 'target'}, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df['context_encoded'] = df['context'].apply(lambda i: stub_coder(ls=i, code_type='encoded'))\n",
    "        df['context_decoded'] = df['context_encoded'].apply(lambda i: stub_coder(ls=i,code_type='decoded'))\n",
    "        df['target_encoded'] = df['target'].apply(lambda i: product_encoder[i])\n",
    "        df['target_decoded'] = df['target_encoded'].apply(lambda i: product_decoder[i])\n",
    "        print(df)\n",
    "        print(df.columns)\n",
    "        self.product_frame = df.copy()\n",
    "        return self.product_frame, product_encoder, product_decoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(batch_size=32, context_size = 4, threshold=1e-5):\n",
    "    product_description_class = ProductDescpMannual()\n",
    "    DF, product_encoder, product_decoder, total_wrd_ls = product_description_class.dataloader_preprocess(limit_it=True, vocab_limit=10000, context_size=4)\n",
    "    df = DF.copy()\n",
    "    num_batches = len(df)//batch_size\n",
    "    corpus_length = len(total_wrd_ls)\n",
    "    df = df.iloc[:num_batches*batch_size]\n",
    "    wrd_counter = Counter(total_wrd_ls)\n",
    "    wrd_freq_dict = {wrd: count/len(total_wrd_ls) for wrd, count in wrd_counter.items()}\n",
    "    wrd_drop_proba = {wrd: 1- np.sqrt(threshold/wrd_freq_dict[wrd]) for wrd in wrd_counter.keys()}\n",
    "    \n",
    "    for idx in range(0, len(df), batch_size):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = ProductDescpMannual()\n",
    "prod.dataloader_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    product_description_class = ProductDescpMannual()\n",
    "    DF, product_encoder, product_decoder, total_wrd_ls = product_description_class.dataloader_preprocess(limit_it=True, vocab_limit=10000, context_size=4)\n",
    "    df = DF.copy()\n",
    "    corpus_length = len(total_wrd_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetBert(torch.utils.data.Dataset):\n",
    "    def __init__(self, max_length=50, min_count=2):\n",
    "        prod = ProductDescpMannual()\n",
    "        full_word_ls, _ = prod.clean_prod()\n",
    "        print(prod.product_frame['product_description'].head())\n",
    "    \n",
    "        # current_vocab = prod.full_word_set\n",
    "        # currnet_corpus = prod.full_word_ls\n",
    "        # current_vocab_size = len(current_vocab) + 1\n",
    "        # self.product_df, self.mannual_word_encoder, self.mannual_word_decoder = prod.dataloader_preprocess()\n",
    "        self.product_df = prod.product_frame\n",
    "\n",
    "\n",
    "        self.label_encoder = prod.major_map_encoder\n",
    "        self.label_decoder = prod.major_map_decoder\n",
    "        self.labels = self.product_df['major_category_encoded'].to_list()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        product_description_counter = Counter(full_word_ls)\n",
    "        main_ls = []\n",
    "        for i in self.product_df['product_description'].copy():\n",
    "            temp_ls = i.split()\n",
    "            temp_ls = [i for i in temp_ls if product_description_counter[i]>min_count]\n",
    "            main_ls.append(' '.join(temp_ls))\n",
    "        self.product_df['product_description'] = main_ls\n",
    "        self.product_descriptions = self.product_df['product_description'].to_list()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.as_tensor(self.labels[idx])\n",
    "        descript = self.product_descriptions[idx]\n",
    "        bert_encoded = self.tokenizer.batch_encode_plus([descript], max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        bert_encoded = {key: torch.LongTensor(value) for key, value in bert_encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            new_description = self.model(**bert_encoded).last_hidden_state.swapaxes(1,2)\n",
    "        \n",
    "        new_description = new_description.squeeze(0)\n",
    "        return new_description, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder {'Home & Garden ': 0, 'Baby & Kids Stuff ': 1, 'DIY Tools & Materials ': 2, 'Music, Films, Books & Games ': 3, 'Phones, Mobile Phones & Telecoms ': 4, 'Clothes, Footwear & Accessories ': 5, 'Other Goods ': 6, 'Health & Beauty ': 7, 'Sports, Leisure & Travel ': 8, 'Appliances ': 9, 'Computers & Software ': 10, 'Office Furniture & Equipment ': 11, 'Video Games & Consoles ': 12}\n",
      "236676\n",
      "1                                                                                             mirror wall art posted nisha dining living room furniture mirrors clocks ornaments wokingham february size xcm\n",
      "2    morphy richards model stainless steel tier stackable food steamer litre capacity litre rice tray easy clean used dishwasher ideal family cookingserves personbrand new never used still original pac...\n",
      "3                                                                                                                                                                           i  collection i do not drive ono\n",
      "4    great reclaimed army ammunition box used coffee side tableoriginal text detail latch leather handle metal corner  fantastic patinareal statement piece one two box sameblack steel prong hairpin leg...\n",
      "5                 new design shannon corner sofa seater available factory sell pricenew amazing design shannon sofa available seater seater sofa available seater sofa available delivery all uk deliver day\n",
      "Name: product_description, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8039, -0.3351,  0.4423,  ...,  0.2718,  0.0503,  0.3203],\n",
      "         [-0.4789, -0.4419, -0.2332,  ..., -0.0547, -0.6744, -0.0808],\n",
      "         [ 0.1831,  0.8881,  0.2569,  ...,  0.4593, -0.0843,  0.4027],\n",
      "         ...,\n",
      "         [-0.1501, -0.3169, -0.0175,  ...,  0.0057,  0.2213,  0.0367],\n",
      "         [-0.4704, -0.1712, -0.7271,  ..., -0.3973, -0.5021, -0.5600],\n",
      "         [ 0.3394, -0.1901,  0.0270,  ...,  0.1082, -0.2261,  0.0184]],\n",
      "\n",
      "        [[-0.2795, -0.0300, -0.8139,  ...,  0.1151,  0.1721,  0.2532],\n",
      "         [-0.3064, -0.1086, -0.2464,  ..., -0.3100, -0.1902, -0.2038],\n",
      "         [ 0.1083,  0.9516,  1.2916,  ...,  0.4343,  0.4590,  0.4705],\n",
      "         ...,\n",
      "         [-0.3358, -0.4791, -0.0049,  ..., -0.1469, -0.2000, -0.2030],\n",
      "         [ 0.0582,  0.1386,  0.1060,  ..., -0.0454, -0.0243, -0.0337],\n",
      "         [ 0.5169, -0.7383,  1.3035,  ...,  0.0371,  0.0780,  0.0073]],\n",
      "\n",
      "        [[-0.3385,  0.2343,  0.4596,  ...,  0.1097,  0.1016,  0.0798],\n",
      "         [ 0.0766,  0.2405,  0.4040,  ..., -0.0018, -0.0545, -0.0348],\n",
      "         [ 0.2107,  0.8018,  0.0266,  ...,  0.3404,  0.3227,  0.3143],\n",
      "         ...,\n",
      "         [-0.2979, -0.0904, -0.2432,  ..., -0.1877, -0.2466, -0.2350],\n",
      "         [ 0.0125,  0.1136, -0.0274,  ..., -0.0724, -0.0954, -0.0976],\n",
      "         [ 0.0663, -0.7247, -0.5953,  ..., -0.1358, -0.0632, -0.0875]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8752,  0.1564, -1.0862,  ...,  0.2092, -0.2326, -0.3090],\n",
      "         [-0.3459,  0.1951,  0.3460,  ..., -0.2816, -0.3541, -0.6504],\n",
      "         [ 0.1623,  0.2726,  0.4647,  ...,  0.4498,  0.2644,  0.0162],\n",
      "         ...,\n",
      "         [-0.5491, -0.6351, -0.5807,  ..., -0.3650,  0.0022,  0.2201],\n",
      "         [ 0.0640,  0.7597, -0.4340,  ..., -0.0883, -0.0156,  0.0452],\n",
      "         [ 0.4048,  0.3834, -0.8718,  ..., -0.1059,  0.0694,  0.0563]],\n",
      "\n",
      "        [[-0.0243,  0.5635, -0.1758,  ...,  0.8188,  0.5825,  0.5862],\n",
      "         [-0.3267, -0.7314, -0.7278,  ..., -0.9260, -0.2331, -0.0108],\n",
      "         [ 0.1462,  1.0469,  0.9244,  ...,  1.1022,  0.1150, -0.0413],\n",
      "         ...,\n",
      "         [-0.1502,  0.2508, -0.2982,  ...,  0.0514,  0.3055, -0.1783],\n",
      "         [-0.1478,  0.4077, -0.3990,  ..., -0.2414, -0.3179, -0.7850],\n",
      "         [ 0.1351,  0.4639, -0.7788,  ..., -0.4076, -0.7512, -0.2119]],\n",
      "\n",
      "        [[-0.3212, -0.1754, -0.4069,  ...,  0.1047,  0.1128,  0.1303],\n",
      "         [-0.0584,  0.7509, -0.5299,  ..., -0.0623, -0.0235, -0.0498],\n",
      "         [ 0.2921,  0.1367,  0.0532,  ...,  0.3052,  0.2873,  0.2029],\n",
      "         ...,\n",
      "         [-0.3928, -0.5475,  0.4122,  ..., -0.5832, -0.5647, -0.4650],\n",
      "         [-0.0564,  0.4843,  0.5522,  ..., -0.3321, -0.3424, -0.3504],\n",
      "         [ 0.3555, -0.4465, -0.4926,  ..., -0.0023, -0.0110, -0.0578]]])\n",
      "tensor([ 4,  7,  5,  3,  6,  2, 12,  9,  7,  8,  4,  7,  1,  1,  3,  4,  6,  0,\n",
      "         8, 10,  4, 11,  5, 10,  5,  1,  3,  1,  2, 10,  8, 12])\n",
      "torch.Size([32, 768, 50])\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDatasetBert()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for i, (description_data, label_category) in enumerate(dataloader):\n",
    "    print(description_data)\n",
    "    print(label_category)\n",
    "    print(description_data.size())\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import  MaxPool1d, AvgPool1d\n",
    "\n",
    "class DescriptionClassifier(Module):\n",
    "    def __init__(self, input_size=768, num_classes=13):\n",
    "        super(DescriptionClassifier, self).__init__()\n",
    "        self.main = nn.Sequential(nn.Conv1d(input_size, 512, kernel_size=3, stride=1, padding=1), nn.Dropout(p=0.2),nn.LeakyReLU(inplace=True),  MaxPool1d(kernel_size=2, stride=2), \n",
    "        nn.Conv1d(512, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), MaxPool1d(kernel_size=2, stride=2), \n",
    "        nn.Conv1d(256, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace = True), nn.AvgPool1d(kernel_size=2, stride=2), \n",
    "        nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=1), nn.LeakyReLU(inplace=True), nn.Flatten(), nn.Linear(192, 64), nn.Tanh(),nn.Linear(64, num_classes))\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.main(inp)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder {'Home & Garden ': 0, 'Baby & Kids Stuff ': 1, 'DIY Tools & Materials ': 2, 'Music, Films, Books & Games ': 3, 'Phones, Mobile Phones & Telecoms ': 4, 'Clothes, Footwear & Accessories ': 5, 'Other Goods ': 6, 'Health & Beauty ': 7, 'Sports, Leisure & Travel ': 8, 'Appliances ': 9, 'Computers & Software ': 10, 'Office Furniture & Equipment ': 11, 'Video Games & Consoles ': 12}\n",
      "236676\n",
      "1                                                                                             mirror wall art posted nisha dining living room furniture mirrors clocks ornaments wokingham february size xcm\n",
      "2    morphy richards model stainless steel tier stackable food steamer litre capacity litre rice tray easy clean used dishwasher ideal family cookingserves personbrand new never used still original pac...\n",
      "3                                                                                                                                                                           i  collection i do not drive ono\n",
      "4    great reclaimed army ammunition box used coffee side tableoriginal text detail latch leather handle metal corner  fantastic patinareal statement piece one two box sameblack steel prong hairpin leg...\n",
      "5                 new design shannon corner sofa seater available factory sell pricenew amazing design shannon sofa available seater seater sofa available seater sofa available delivery all uk deliver day\n",
      "Name: product_description, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDatasetBert()\n",
    "n_epochs=10\n",
    "batch_size=32\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder {'Home & Garden ': 0, 'Baby & Kids Stuff ': 1, 'DIY Tools & Materials ': 2, 'Music, Films, Books & Games ': 3, 'Phones, Mobile Phones & Telecoms ': 4, 'Clothes, Footwear & Accessories ': 5, 'Other Goods ': 6, 'Health & Beauty ': 7, 'Sports, Leisure & Travel ': 8, 'Appliances ': 9, 'Computers & Software ': 10, 'Office Furniture & Equipment ': 11, 'Video Games & Consoles ': 12}\n",
      "236676\n",
      "1                                                                                             mirror wall art posted nisha dining living room furniture mirrors clocks ornaments wokingham february size xcm\n",
      "2    morphy richards model stainless steel tier stackable food steamer litre capacity litre rice tray easy clean used dishwasher ideal family cookingserves personbrand new never used still original pac...\n",
      "3                                                                                                                                                                           i  collection i do not drive ono\n",
      "4    great reclaimed army ammunition box used coffee side tableoriginal text detail latch leather handle metal corner  fantastic patinareal statement piece one two box sameblack steel prong hairpin leg...\n",
      "5                 new design shannon corner sofa seater available factory sell pricenew amazing design shannon sofa available seater seater sofa available seater sofa available delivery all uk deliver day\n",
      "Name: product_description, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torchvision\n",
    "from torchbearer import Trial\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "from torchbearer.callbacks import TensorBoard\n",
    "from torch.nn import Module\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "product_model = DescriptionClassifier()\n",
    "dataset = TextDatasetBert()\n",
    "n_epochs=10\n",
    "batch_size=32\n",
    "train_prop = 0.75\n",
    "train_end = int(train_prop*len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset=dataset, lengths=[train_end, len(dataset)-train_end])\n",
    "dataset_dict = {'train': train_dataset, 'eval': test_dataset}\n",
    "\n",
    "dataloader_dict = {i: torch.utils.data.DataLoader(dataset_dict[i], batch_size=batch_size, shuffle=True) for i in ['train', 'eval']}\n",
    "\n",
    "opt = optim.SGD\n",
    "optimizer =  opt(product_model.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'Model training and testing function'\n",
    "def train_model(model=product_model, optimizer=optimizer, dataset=dataset,loss_type = criterion, num_epochs = 1, mode_scheduler=None, batch_size = 24, train_proportion=0.75):\n",
    "    best_accuracy = 0 # May be changed at end of each \"for phase block\"\n",
    "    start = time.time()\n",
    "    writer = SummaryWriter()\n",
    "    train_end = int(train_proportion*len(dataset))\n",
    "    train_dataset, test_dataset = random_split(dataset=dataset, lengths=[train_end, len(dataset)-train_end])\n",
    "    dataset_dict = {'train': train_dataset, 'eval': test_dataset}\n",
    "    dataloader_dict = {i: torch.utils.data.DataLoader(dataset_dict[i], batch_size=batch_size, shuffle=True) for i in ['train', 'eval']}\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'eval']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for batch_num, (inputs, labels) in enumerate(dataloader_dict[phase], start=1):\n",
    "                optimizer.zero_grad() # Gradients reset to zero at beginning of both training and evaluation phase\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # print(inputs)\n",
    "                    # print(inputs.size())\n",
    "                    outputs = model(inputs)\n",
    "                    #outputs = torch.softmax(outputs, dim=1)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    # print(preds)\n",
    "                    loss = loss_type(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() #Calculates gradients\n",
    "                        optimizer.step()\n",
    "\n",
    "                if batch_num%20==0:\n",
    "                    '''Writer functions for batch'''\n",
    "                    # writer.add_figure('Predictions vs Actual',plot_classes_preds(input_arr=inputs, lab=labels, model=model))\n",
    "                    writer.add_scalar(f'Accuracy for phase {phase} by batch number', preds.eq(labels).sum()/batch_size, batch_num)\n",
    "                    writer.add_scalar(f'Average loss for phase {phase} by batch number', loss.item(), batch_num)\n",
    "\n",
    "                running_corrects = running_corrects + preds.eq(labels).sum()\n",
    "                running_loss = running_loss + (loss.item()*inputs.size(0))\n",
    "\n",
    "            if phase=='train' and (mode_scheduler is not None):\n",
    "                mode_scheduler.step()\n",
    "\n",
    "            '''Writer functions for epoch'''\n",
    "            epoch_loss = running_loss / len(dataset_dict[phase])\n",
    "            print(f'Size of dataset for phase {phase}', dataset_dict[phase])\n",
    "            epoch_acc = running_corrects / len(dataset_dict[phase])\n",
    "            writer.add_scalar(f'Accuracy by epoch phase {phase}', epoch_acc, epoch)\n",
    "            print(f'{phase.title()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            writer.add_scalar(f'Average loss by epoch phase {phase}', epoch_loss, epoch)\n",
    "            print(f'Done {epoch} epoch(s)')\n",
    "\n",
    "            if phase == 'eval' and epoch_acc > best_accuracy:\n",
    "                best_accuracy = epoch_acc\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                print(f'Best val Acc: {best_accuracy:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    torch.save(model.state_dict(), 'prodcut_model.pt')\n",
    "    time_diff = time.time()-start\n",
    "    print(f'Time taken for model to run: {(time_diff//60)} minutes and {(time_diff%60):.0f} seconds')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37275937203149406\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 2.4894 Acc: 0.1399\n",
      "Done 0 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 2.1980 Acc: 0.2372\n",
      "Done 0 epoch(s)\n",
      "Best val Acc: 0.2372\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 1.9836 Acc: 0.2994\n",
      "Done 1 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.7094 Acc: 0.3823\n",
      "Done 1 epoch(s)\n",
      "Best val Acc: 0.3823\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 1.6584 Acc: 0.4190\n",
      "Done 2 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.6218 Acc: 0.3976\n",
      "Done 2 epoch(s)\n",
      "Best val Acc: 0.3976\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 1.4941 Acc: 0.4816\n",
      "Done 3 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.4396 Acc: 0.4960\n",
      "Done 3 epoch(s)\n",
      "Best val Acc: 0.4960\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 1.2235 Acc: 0.5891\n",
      "Done 4 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2985 Acc: 0.5739\n",
      "Done 4 epoch(s)\n",
      "Best val Acc: 0.5739\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 1.1363 Acc: 0.6215\n",
      "Done 5 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2669 Acc: 0.5882\n",
      "Done 5 epoch(s)\n",
      "Best val Acc: 0.5882\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 1.0616 Acc: 0.6566\n",
      "Done 6 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2372 Acc: 0.6109\n",
      "Done 6 epoch(s)\n",
      "Best val Acc: 0.6109\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.9819 Acc: 0.6822\n",
      "Done 7 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2562 Acc: 0.6098\n",
      "Done 7 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.8176 Acc: 0.7419\n",
      "Done 8 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.1506 Acc: 0.6490\n",
      "Done 8 epoch(s)\n",
      "Best val Acc: 0.6490\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.7510 Acc: 0.7626\n",
      "Done 9 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2032 Acc: 0.6354\n",
      "Done 9 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.6951 Acc: 0.7897\n",
      "Done 10 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2282 Acc: 0.6422\n",
      "Done 10 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.6468 Acc: 0.8005\n",
      "Done 11 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2730 Acc: 0.6451\n",
      "Done 11 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.5348 Acc: 0.8481\n",
      "Done 12 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.1816 Acc: 0.6593\n",
      "Done 12 epoch(s)\n",
      "Best val Acc: 0.6593\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.4991 Acc: 0.8631\n",
      "Done 13 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2081 Acc: 0.6621\n",
      "Done 13 epoch(s)\n",
      "Best val Acc: 0.6621\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.4639 Acc: 0.8769\n",
      "Done 14 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.1999 Acc: 0.6655\n",
      "Done 14 epoch(s)\n",
      "Best val Acc: 0.6655\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.4368 Acc: 0.8849\n",
      "Done 15 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2270 Acc: 0.6598\n",
      "Done 15 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3830 Acc: 0.9073\n",
      "Done 16 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2253 Acc: 0.6610\n",
      "Done 16 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3646 Acc: 0.9143\n",
      "Done 17 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2332 Acc: 0.6644\n",
      "Done 17 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3537 Acc: 0.9154\n",
      "Done 18 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2396 Acc: 0.6570\n",
      "Done 18 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3405 Acc: 0.9196\n",
      "Done 19 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2362 Acc: 0.6610\n",
      "Done 19 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3213 Acc: 0.9266\n",
      "Done 20 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2535 Acc: 0.6604\n",
      "Done 20 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3143 Acc: 0.9302\n",
      "Done 21 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2452 Acc: 0.6604\n",
      "Done 21 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3104 Acc: 0.9312\n",
      "Done 22 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2569 Acc: 0.6542\n",
      "Done 22 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.3055 Acc: 0.9321\n",
      "Done 23 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2580 Acc: 0.6553\n",
      "Done 23 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.2970 Acc: 0.9352\n",
      "Done 24 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2604 Acc: 0.6570\n",
      "Done 24 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.2942 Acc: 0.9378\n",
      "Done 25 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2590 Acc: 0.6570\n",
      "Done 25 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.2932 Acc: 0.9334\n",
      "Done 26 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2616 Acc: 0.6593\n",
      "Done 26 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.2911 Acc: 0.9352\n",
      "Done 27 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2620 Acc: 0.6581\n",
      "Done 27 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.2864 Acc: 0.9393\n",
      "Done 28 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2632 Acc: 0.6576\n",
      "Done 28 epoch(s)\n",
      "Size of dataset for phase train <torch.utils.data.dataset.Subset object at 0x7fcfb08dc940>\n",
      "Train Loss: 0.2868 Acc: 0.9403\n",
      "Done 29 epoch(s)\n",
      "Size of dataset for phase eval <torch.utils.data.dataset.Subset object at 0x7fcfb08dcaf0>\n",
      "Eval Loss: 1.2635 Acc: 0.6576\n",
      "Done 29 epoch(s)\n",
      "Time taken for model to run: 809.0 minutes and 14 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescriptionClassifier(\n",
       "  (main): Sequential(\n",
       "    (0): Conv1d(768, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv1d(256, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (10): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "    (13): Linear(in_features=192, out_features=64, bias=True)\n",
       "    (14): Tanh()\n",
       "    (15): Linear(in_features=64, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=30\n",
    "step_sz = 4\n",
    "num_steps = num_epochs//step_sz\n",
    "initial_lr = 0.1\n",
    "fin_lr = 0.0001\n",
    "gamma_mult = (fin_lr/initial_lr)**(1/num_steps)\n",
    "print(gamma_mult)\n",
    "step_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=4, gamma=gamma_mult) \n",
    "train_model(num_epochs=num_epochs, mode_scheduler=step_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scaper_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ca6e4f64349b37e43d103d34f8b7f64a464de9243649aae08dfeb58b2445472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
