{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/aj8/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from collections import Counter, defaultdict\n",
    "from ntpath import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import column\n",
    "import xlsxwriter\n",
    "import os\n",
    "import seaborn as sns\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import contractions \n",
    "from pathlib import Path\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.io import imread, imshow\n",
    "from skimage import io\n",
    "import json\n",
    "from skimage import img_as_float\n",
    "from skimage.transform import rescale, resize\n",
    "from itertools import product\n",
    "from PIL import Image\n",
    "from clean_tabular import CleanData, CleanImages\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "class ProductDescpMannual(CleanData):\n",
    "    def __init__(self):\n",
    "        super().__init__(tab_names=['Products'])\n",
    "        self.label_len = len(self.major_map_decoder)\n",
    "        self.product_frame = self.table_dict['Products'].copy()\n",
    "\n",
    "    def clean_prod(self, col='product_description'):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_lemmitizer = WordNetLemmatizer()\n",
    "        punct_re = re.compile(r'[^A-Za-z \\n-]') #[^A-Za-z0-9 .]\n",
    "        self.product_frame[col] = self.product_frame[col].copy().str.replace(punct_re, '')\n",
    "        all_text_ls = []\n",
    "        main_ls = []\n",
    "        for i in self.product_frame[col]:\n",
    "            dum_ls = i.split()\n",
    "            dum_ls = [i for i in dum_ls if i not in  stop_words]\n",
    "            dum_ls = [i.replace('\\n', ' ') for i in dum_ls]\n",
    "            dum_ls = [i.strip('\\-') for i in dum_ls]\n",
    "            dum_ls = [word_lemmitizer.lemmatize(i) for i in dum_ls]\n",
    "            dum_ls = [contractions.fix(j) for j in dum_ls]\n",
    "            dum_ls = [i for i in dum_ls if i.isalpha] # and len(i)>2\n",
    "            main_ls.append(' '.join(dum_ls))\n",
    "            # print(' '.join(dum_ls))\n",
    "            all_text_ls.extend(dum_ls)\n",
    "        all_text_ls = [i.lower() for i in all_text_ls]\n",
    "\n",
    "        # wrd_counter = Counter(all_text_ls)\n",
    "        # wrd_freq_dict = {wrd: count/len(wrd_counter) for wrd, count in wrd_counter.items()}\n",
    "        # wrd_drop_proba = {wrd: 1- np.sqrt(threshold/wrd_freq_dict[wrd]) for wrd in wrd_counter.keys()}\n",
    "        # main_ls = []\n",
    "        # for i in self.product_frame[col]:\n",
    "        #     dum_ls = i.split()\n",
    "\n",
    "\n",
    "        self.product_frame[col] = main_ls \n",
    "        self.product_frame[col] = self.product_frame[col].apply(lambda i: i.lower())\n",
    "        print(len(all_text_ls))\n",
    "        return all_text_ls, len(all_text_ls)\n",
    "    \n",
    "    def word_freq(self, col='product_description', num_items=100):\n",
    "        all_words = self.clean_prod(col=col)[0]\n",
    "        count_ls = Counter(all_words)\n",
    "        if num_items is not None:\n",
    "            count_dict = {i[0]: i[1] for i in count_ls.most_common(num_items)}\n",
    "        else:\n",
    "            count_dict = {i[0]: i[1] for i in count_ls.most_common()}\n",
    "        return count_dict\n",
    "\n",
    "    def get_word_set(self, col='product_description'):\n",
    "        if col=='product_name':\n",
    "            self.clean_prod_name(col=col)\n",
    "        self.clean_prod(col=col)\n",
    "        self.full_word_ls = []\n",
    "        for i in self.product_frame[col]:\n",
    "            self.full_word_ls.extend(i.split())\n",
    "        self.full_word_set = list(set(self.full_word_ls))\n",
    "        return self.full_word_ls, self.full_word_set ,len(self.full_word_set)+1\n",
    "\n",
    "    def vocab_encoder(self, col='product_description', limit_it=False, vocab_limit=30000):\n",
    "        word_ls, word_set, full_vocab_size = self.get_word_set(col)\n",
    "        if limit_it==False:\n",
    "            vocab_size = full_vocab_size\n",
    "            self.word_encoder = defaultdict(lambda: vocab_size-1)\n",
    "            current_word_encoder = {key[0]: integer for integer, key in enumerate(Counter(word_ls).most_common()[:vocab_size-1])}\n",
    "            self.word_encoder['<UNKNOWN>'] = vocab_size-1\n",
    "            self.word_encoder.update(current_word_encoder)\n",
    "            self.word_decoder = {val: key for key, val in self.word_encoder.items()}\n",
    "        else:\n",
    "            vocab_size=vocab_limit\n",
    "            self.word_encoder = defaultdict(lambda: vocab_size)\n",
    "            current_word_encoder = {key[0]: integer for integer, key in enumerate(Counter(word_ls).most_common()[:vocab_size])}\n",
    "            self.word_encoder['<UNKNOWN>'] = vocab_size\n",
    "            self.word_encoder.update(current_word_encoder)\n",
    "            self.word_decoder = {val: key for key, val in self.word_encoder.items()}\n",
    "        \n",
    "        # self.word_encoder = {val: key for key, val in self.word_decoder.items()}\n",
    "        return self.word_encoder, self.word_decoder, self.product_frame\n",
    "\n",
    "    \n",
    "    def dataloader_preprocess(self, limit_it=True, vocab_lim=10000, context_size=5):\n",
    "        product_encoder, product_decoder, DF = self.vocab_encoder(limit_it=limit_it, vocab_limit=vocab_lim)\n",
    "        df = DF.copy()\n",
    "        init_ls = []\n",
    "        for i in range(len(df)):\n",
    "            prod_descript = df['product_description'].iloc[i].split()\n",
    "            init_ls.append([\n",
    "                [\n",
    "                    list(np.array([[prod_descript[i - j], prod_descript[i+j]] for j in range(1, context_size+1)]).flatten()),\n",
    "                    prod_descript[i]\n",
    "                ]\n",
    "                for i in range(context_size, len(prod_descript)-context_size)\n",
    "            ])\n",
    "        df['original_context_target'] = init_ls\n",
    "        df = df.explode('original_context_target').reset_index()\n",
    "        for idx, val in enumerate(df['original_context_target']):\n",
    "            if type(val) is not list:\n",
    "                df.drop(idx, axis=0, inplace=True)\n",
    "        \n",
    "        def stub_coder(ls, code_type='encoded'):\n",
    "            code_dict = {'encoded': product_encoder, 'decoded': product_decoder}\n",
    "            new_ls = []\n",
    "            for i in ls.copy():\n",
    "                new_ls.append(code_dict[code_type][i])\n",
    "            return new_ls\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(df['original_context_target'].tolist())], axis=1)\n",
    "        df.rename(columns={0: 'context', 1: 'target'}, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df['context_encoded'] = df['context'].apply(lambda i: stub_coder(ls=i, code_type='encoded'))\n",
    "        df['context_decoded'] = df['context_encoded'].apply(lambda i: stub_coder(ls=i,code_type='decoded'))\n",
    "        df['target_encoded'] = df['target'].apply(lambda i: product_encoder[i])\n",
    "        df['target_decoded'] = df['target_encoded'].apply(lambda i: product_decoder[i])\n",
    "        print(df)\n",
    "        print(df.columns)\n",
    "        self.product_frame = df.copy()\n",
    "        return self.product_frame, product_encoder, product_decoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(batch_size=32, context_size = 4, threshold=1e-5):\n",
    "    product_description_class = ProductDescpMannual()\n",
    "    DF, product_encoder, product_decoder, total_wrd_ls = product_description_class.dataloader_preprocess(limit_it=True, vocab_limit=10000, context_size=4)\n",
    "    df = DF.copy()\n",
    "    num_batches = len(df)//batch_size\n",
    "    corpus_length = len(total_wrd_ls)\n",
    "    df = df.iloc[:num_batches*batch_size]\n",
    "    wrd_counter = Counter(total_wrd_ls)\n",
    "    wrd_freq_dict = {wrd: count/len(total_wrd_ls) for wrd, count in wrd_counter.items()}\n",
    "    wrd_drop_proba = {wrd: 1- np.sqrt(threshold/wrd_freq_dict[wrd]) for wrd in wrd_counter.keys()}\n",
    "    \n",
    "    for idx in range(0, len(df), batch_size):\n",
    "        pa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = ProductDescpMannual()\n",
    "prod.dataloader_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    product_description_class = ProductDescpMannual()\n",
    "    DF, product_encoder, product_decoder, total_wrd_ls = product_description_class.dataloader_preprocess(limit_it=True, vocab_limit=10000, context_size=4)\n",
    "    df = DF.copy()\n",
    "    corpus_length = len(total_wrd_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/aj8/Downloads/AICore/facebook_mkt/dataload_wrd2vec.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aj8/Downloads/AICore/facebook_mkt/dataload_wrd2vec.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTextDatasetBert\u001b[39;00m(torch\u001b[39m.\u001b[39mutls\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aj8/Downloads/AICore/facebook_mkt/dataload_wrd2vec.ipynb#ch0000010?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, max_length, min_count\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aj8/Downloads/AICore/facebook_mkt/dataload_wrd2vec.ipynb#ch0000010?line=2'>3</a>\u001b[0m         prod \u001b[39m=\u001b[39m ProductDescpMannual()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class TextDatasetBert(torch.utls.data.Dataset):\n",
    "    def __init__(self, max_length, min_count=2):\n",
    "        prod = ProductDescpMannual()\n",
    "        self.product_df, self.mannual_word_encoder, self.mannual_word_decoder = prod.dataloader_preprocess()\n",
    "        current_vocab = prod.full_word_set\n",
    "        currnet_corpus = prod.full_word_ls\n",
    "        current_vocab_size = len(current_vocab) + 1\n",
    "\n",
    "        self.label_encoder = prod.major_map_encoder\n",
    "        self.label_decoder = prod.major_map_decoder\n",
    "        self.labels = self.product_df['major_category_encoded'].to_list()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        product_description_counter = Counter(current_corpus)\n",
    "        main_ls = []\n",
    "        for i in self.product_df['product_description'].copy():\n",
    "            temp_ls = i.split()\n",
    "            temp_ls = [i for i in temp_ls if product_description_counter[i]>min_count]\n",
    "            main_ls.append(' '.join(temp_ls))\n",
    "        self.product_df['product_description'] = main_ls\n",
    "        self.product_descriptions = self.product_df['product_description'].to_list()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = torch.as_tensor(self.labels[idx])\n",
    "        descript = self.product_descriptions[idx]\n",
    "        bert_encoded = self.tokenizer.batch_encode_plus([descript], max_lenght=self.max_length, padding='max_length', truncation=True)\n",
    "        bert_encoded = {key: torch.LongTensor(value) for key, value in bert_encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            new_description = self.model(**bert_encoded).last_hidden_state.swapaxes(1,2)\n",
    "        \n",
    "        new_description = new_description.squeeze(0)\n",
    "        return new_description, label\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 2), ('c', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scaper_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ca6e4f64349b37e43d103d34f8b7f64a464de9243649aae08dfeb58b2445472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
